<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>备忘</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sibyl233.github.io/"/>
  <updated>2020-02-11T13:04:05.377Z</updated>
  <id>http://sibyl233.github.io/</id>
  
  <author>
    <name>Sibyl</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Zotero简明使用指南</title>
    <link href="http://sibyl233.github.io/2020/01/08/Zotero%E7%AE%80%E6%98%8E%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <id>http://sibyl233.github.io/2020/01/08/Zotero简明使用指南/</id>
    <published>2020-01-08T08:22:23.000Z</published>
    <updated>2020-02-11T13:04:05.377Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h4 id="下载与基本设置"><a href="#下载与基本设置" class="headerlink" title="下载与基本设置"></a>下载与基本设置</h4><a id="more"></a><h5 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h5><p><a href="https://www.zotero.org/download/" target="_blank" rel="noopener">官网下载</a>Zotero和Zotero Connector。前者是管理文献的本体；后者是用于抓取条目和相关附件的浏览器插件。</p><h5 id="设置同步"><a href="#设置同步" class="headerlink" title="设置同步"></a>设置同步</h5><p>Zotero本身自带同步功能，但是自带空间有限，故可以使用webdav连接到其他网盘，以坚果云为例：<a href="http://help.jianguoyun.com/?p=3168" target="_blank" rel="noopener">如何在Zotero中设置webdav连接到坚果云？</a></p><h5 id="设置Zotfile"><a href="#设置Zotfile" class="headerlink" title="设置Zotfile"></a>设置Zotfile</h5><p>Zotfile是一款插件，配合Zotero使用可以更好地管理（批量命名和移除等）文献附件：<a href="http://zotfile.com/#how-to-install--set-up-zotfile" target="_blank" rel="noopener">下载和设置</a>。</p><h4 id="其它功能"><a href="#其它功能" class="headerlink" title="其它功能"></a>其它功能</h4><h5 id="导出参考文献（Word-Latex）"><a href="#导出参考文献（Word-Latex）" class="headerlink" title="导出参考文献（Word/Latex）"></a>导出参考文献（Word/Latex）</h5><p>若<a href="https://www.zotero.org/support/creating_bibliographies" target="_blank" rel="noopener">不使用插件</a>，❶直接拖拽条目至文本编辑器/使用快捷键即可实现单条目的快速复制；❷在Zotero中选取所需条目，右键选择<code>由所选条目创建引文目录</code>即可实现多条目的快速复制。<br>若<a href="https://www.zotero.org/support/word_processor_integration" target="_blank" rel="noopener">使用插件</a>，以Word为例，在成功安装插件的前提下Word菜单栏中会出现Zotero选项卡，可方便地添加引用和参考文献。</p><h5 id="Papership（ipad-iphone端）"><a href="#Papership（ipad-iphone端）" class="headerlink" title="Papership（ipad/iphone端）"></a>Papership（ipad/iphone端）</h5><p>Zotero只有PC端应用，配合Papership使用可以弥补该不足。用Zotero账户登录Papership应用，并<a href="http://shazino.freshdesk.com/support/solutions/articles/112464-how-to-sync-papership-and-zotero-" target="_blank" rel="noopener">设置</a>Zotero File Hosting（与步骤1.2设置同步类似）就可以在平板和手机上同步阅读文献了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;下载与基本设置&quot;&gt;&lt;a href=&quot;#下载与基本设置&quot; class=&quot;headerlink&quot; title=&quot;下载与基本设置&quot;&gt;&lt;/a&gt;下载与基本设置&lt;/h4&gt;
    
    </summary>
    
      <category term="指南" scheme="http://sibyl233.github.io/categories/%E6%8C%87%E5%8D%97/"/>
    
    
  </entry>
  
  <entry>
    <title>语义分割方法整理</title>
    <link href="http://sibyl233.github.io/2019/09/23/Semantic-Segmentation/"/>
    <id>http://sibyl233.github.io/2019/09/23/Semantic-Segmentation/</id>
    <published>2019-09-23T05:43:58.000Z</published>
    <updated>2020-02-11T13:03:47.932Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h4 id="Basis"><a href="#Basis" class="headerlink" title="Basis"></a>Basis</h4><a id="more"></a><ul><li>FCN<ul><li>[FCN] Fully Convolutional Networks for Semantic Segmentation</li></ul></li><li>DeepLab<ul><li>[DeepLab v1] Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs </li><li>[DeepLab v2] DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</li><li>[DeepLab v3] Rethinking Atrous Convolution for Semantic Image Segmentation </li><li>[DeepLab v3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</li></ul></li><li>Encoder-decoder Architecture<ul><li>[U-Net] U-Net: Convolutional Networks for Biomedical Image Segmentation </li><li>[SegNet] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation</li><li>[DeconvNet] Learning Deconvolution Network for Semantic Segmentation</li><li>[RefineNet] RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</li></ul></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h5><ul><li>[CRFasRNN] Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</li><li>[MRF] Semantic Image Segmentation via Deep Parsing Network</li><li>[GRF] Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs</li></ul><h5 id="Atrous-Dilated-Convolution"><a href="#Atrous-Dilated-Convolution" class="headerlink" title="Atrous/Dilated Convolution"></a>Atrous/Dilated Convolution</h5><ul><li>[DUC-HDC] Understanding Convolution for Semantic Segmentation</li><li>[DRN] Dilated Residual Networks</li><li>Smoothed Dilated Convolutions for Improved Dense Prediction</li><li>Efficient Smoothing of Dilated Convolutions for Image Segmentation</li><li>[FastFCN] FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation</li></ul><h5 id="Context-Aggregation"><a href="#Context-Aggregation" class="headerlink" title="Context Aggregation"></a>Context Aggregation</h5><ul><li>Pooling<ul><li>[ParseNet] ParseNet: Looking Wider to See Better</li><li>[PSPNet] Pyramid Scene Parsing Network</li><li>[DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes</li><li>[VortexPooling] Vortex Pooling: Improving Context Representation in Semantic Segmentation</li></ul></li><li>Large Kernel<ul><li>[GCN] Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network</li><li>[ExFuse] ExFuse: Enhancing Feature Fusion for Semantic Segmentation</li></ul></li></ul><h5 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h5><ul><li>Channel Reweighting<ul><li>[DFN] Learning a Discriminative Feature Network for Semantic Segmentation</li><li>[EncNet] Context Encoding for Semantic Segmentation[</li><li>[SENet] Squeeze-and-Excitation Networks</li><li>Pyramid Attention Network for Semantic Segmentation</li></ul></li><li>Spatial Attention<ul><li>[OCNet] OCNet: Object Context Network for Scene Parsing</li><li>[DANet] Dual Attention Networks for Multimodal Reasoning and Matching</li><li>[PSANet] PSANet: Point-wise Spatial Attention Network for Scene Parsing</li><li>[CCNet] CCNet: Criss-Cross Attention for Semantic Segmentation</li></ul></li></ul><h5 id="Graph-Convolution"><a href="#Graph-Convolution" class="headerlink" title="Graph Convolution"></a>Graph Convolution</h5><ul><li>[GloRe] Graph-Based Global Reasoning Networks</li><li>Beyond Grids: Learning Graph Representations for Visual Recognition</li></ul><h4 id="Real-time-Method"><a href="#Real-time-Method" class="headerlink" title="Real-time Method"></a>Real-time Method</h4><h5 id="Convolution-Factorization"><a href="#Convolution-Factorization" class="headerlink" title="Convolution Factorization"></a>Convolution Factorization</h5><ul><li>[ENet] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</li><li>[ERFNet]  ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</li><li>[ESPNet] ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</li><li>[ESNet] ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation</li><li>[LEDNet] LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation</li><li>[DABNet] DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation</li></ul><h5 id="Multi-branch"><a href="#Multi-branch" class="headerlink" title="Multi-branch"></a>Multi-branch</h5><ul><li>[ICNet] ICNet for Real-Time Semantic Segmentation on High-Resolution Images</li><li>[ContextNet] ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time</li><li>[BiSeNet] BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation</li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li>语义分割论文整理 [<a href="https://zhangbin0917.github.io/2018/09/18/Semantic-Segmentation/" target="_blank" rel="noopener">blog</a>]</li><li>Survey on semantic segmentation using deep learning techniques [<a href="https://www.sciencedirect.com/science/article/pii/S092523121930181X" target="_blank" rel="noopener">paper</a>]</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Basis&quot;&gt;&lt;a href=&quot;#Basis&quot; class=&quot;headerlink&quot; title=&quot;Basis&quot;&gt;&lt;/a&gt;Basis&lt;/h4&gt;
    
    </summary>
    
      <category term="清单" scheme="http://sibyl233.github.io/categories/%E6%B8%85%E5%8D%95/"/>
    
    
  </entry>
  
  <entry>
    <title>道路场景语义分割</title>
    <link href="http://sibyl233.github.io/2019/07/11/%E9%81%93%E8%B7%AF%E5%9C%BA%E6%99%AF%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    <id>http://sibyl233.github.io/2019/07/11/道路场景语义分割/</id>
    <published>2019-07-11T03:35:50.000Z</published>
    <updated>2020-02-11T12:33:24.647Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><a id="more"></a><ul><li><a href="http://apolloscape.auto/scene.html" target="_blank" rel="noopener">Apolloscape Scene Parsing</a></li><li><a href="https://bair.berkeley.edu/blog/2018/05/30/bdd/" target="_blank" rel="noopener">BDD100k</a></li><li><a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/" target="_blank" rel="noopener">CamVid</a></li><li><a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener">Cityscapes</a></li><li><a href="http://www.6d-vision.com/scene-labeling" target="_blank" rel="noopener">Daimler Urban Segmentation</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_semantics.php" target="_blank" rel="noopener">Kitti</a></li><li><a href="https://www.mapillary.com/dataset/" target="_blank" rel="noopener">Mapillary Vistas</a></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><div class="table-container"><table><thead><tr><th>Method</th><th>Cityscapes(%)</th><th>CamVid(%)</th><th>Publication</th></tr></thead><tbody><tr><td>FCN-8s</td><td>65.3</td><td></td><td>CVPR2015</td></tr><tr><td>SegNet</td><td></td><td>50.2</td><td>arXiv15</td></tr><tr><td>DeepLab v2</td><td>70.4</td><td></td><td>PAMI2018</td></tr><tr><td>G-FRNet</td><td></td><td>68.0</td><td>CVPR2017</td></tr><tr><td>FRRN B</td><td>71.8</td><td></td><td>CVPR2017</td></tr><tr><td>RefineNet</td><td>73.6</td><td></td><td>CVPR2017</td></tr><tr><td>GCN</td><td>76.9</td><td></td><td>CVPR2017</td></tr><tr><td>TKCN</td><td>79.5</td><td></td><td>ICME2019</td></tr><tr><td>DUC-HDC</td><td>80.1</td><td></td><td>WACV2018</td></tr><tr><td>PSANet</td><td>80.1</td><td></td><td>ECCV2018</td></tr><tr><td>PSPNet</td><td>80.2</td><td></td><td>CVPR2017</td></tr><tr><td>DDSC</td><td></td><td>70.9</td><td>CVPR2018</td></tr><tr><td>DFN</td><td>80.3</td><td></td><td>CVPR2018</td></tr><tr><td>DenseASPP</td><td>80.6</td><td></td><td>CVPR2018</td></tr><tr><td>LDN</td><td>80.6</td><td>78.1</td><td>arXiv19</td></tr><tr><td>GloRe</td><td>80.9</td><td></td><td>CVPR2019</td></tr><tr><td>OCNet</td><td>81.2</td><td></td><td>arXiv18</td></tr><tr><td>DeepLab v3</td><td>81.3</td><td></td><td>arXiv17</td></tr><tr><td>CCNet</td><td>81.4</td><td></td><td>arXiv18</td></tr><tr><td>DAN</td><td>81.5</td><td></td><td>CVPR2019</td></tr><tr><td>HRNetV2</td><td>81.6</td><td></td><td>arXiv19</td></tr><tr><td>CaseNet</td><td>81.9</td><td></td><td>arXiv19</td></tr><tr><td>DeepLab v3+</td><td>82.1</td><td></td><td>ECCV2018</td></tr><tr><td>GFF</td><td>82.3</td><td></td><td>arXiv19</td></tr><tr><td>DPC</td><td>82.7</td><td></td><td>NIPS2018</td></tr><tr><td>GSCNN</td><td>82.8</td><td></td><td>ICCV2019</td></tr></tbody></table></div><h4 id="Real-time-Method"><a href="#Real-time-Method" class="headerlink" title="Real-time Method"></a>Real-time Method</h4><div class="table-container"><table><thead><tr><th>Method</th><th>WxH</th><th>mIOU(%)</th><th>FPS</th><th>GFLOPs</th><th>Param(M)</th></tr></thead><tbody><tr><td>ENet</td><td>1920x1080</td><td>58.3</td><td>21.6</td><td>3.83</td><td>0.37</td></tr><tr><td>SQ</td><td>2048x1024</td><td>59.8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TwoColumn</td><td>1024x512</td><td>72.9</td><td>14.7</td><td>-</td><td>-</td></tr><tr><td>CGNet</td><td>2048x1024</td><td>64.8</td><td>17.6</td><td>6</td><td>0.5</td></tr><tr><td>ESPNet</td><td>-</td><td>60.3</td><td>4</td><td>-</td><td>0.4</td></tr><tr><td>ERFNet</td><td>-</td><td>68</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ICNet</td><td>2048x1024</td><td>69.5</td><td>30.3</td><td>-</td><td>-</td></tr><tr><td>BiSeNet</td><td>1536x768</td><td>68.4</td><td>105.8</td><td>2.9</td><td>5.8</td></tr><tr><td></td><td>1536x768</td><td>74.7</td><td>65.5</td><td>10.8</td><td>49</td></tr><tr><td>ESNet</td><td>-</td><td>70.7</td><td>63</td><td>-</td><td>1.66</td></tr><tr><td>DFANet</td><td>1024x1024</td><td>71.3</td><td>100</td><td>3.4</td><td>7.8</td></tr><tr><td>LEDNet</td><td>1024x512</td><td>70.1</td><td>71</td><td>-</td><td>0.94</td></tr><tr><td>DABNet</td><td>1024x512</td><td>70.1</td><td>104.2</td><td>-</td><td>0.76</td></tr><tr><td>SqueezeNAS</td><td>1024x512</td><td>72.5</td><td>-</td><td>-</td><td>1.9</td></tr></tbody></table></div><p>*上表为文献中各方法在Cityscapes上的性能</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Datasets&quot;&gt;&lt;a href=&quot;#Datasets&quot; class=&quot;headerlink&quot; title=&quot;Datasets&quot;&gt;&lt;/a&gt;Datasets&lt;/h4&gt;
    
    </summary>
    
      <category term="清单" scheme="http://sibyl233.github.io/categories/%E6%B8%85%E5%8D%95/"/>
    
    
  </entry>
  
  <entry>
    <title>一个测试</title>
    <link href="http://sibyl233.github.io/2018/05/11/%E4%B8%80%E4%B8%AA%E6%B5%8B%E8%AF%95/"/>
    <id>http://sibyl233.github.io/2018/05/11/一个测试/</id>
    <published>2018-05-11T11:20:15.000Z</published>
    <updated>2019-02-05T07:46:59.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ヾ(๑╹◡╹)ﾉ”  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="胡言" scheme="http://sibyl233.github.io/categories/%E8%83%A1%E8%A8%80/"/>
    
    
  </entry>
  
</feed>
