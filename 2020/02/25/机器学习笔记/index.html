<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000">
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top">
  
  
  <title>机器学习笔记 | Hexo</title>
  <meta name="description" content="1 引言1.1 机器学习定义第一个定义——Arthur Samuel对机器学习的定义：  机器学习是这样的领域，它赋予计算机学习的能力，这种学习能力不是通过显著式编程获得的。   显著式编程：比如发一系列指令（左转-开门-右转……）让机器人到教室门外的咖啡机前冲咖啡，其劣势是需要把机器人所处的环境调查得一清二楚， 非显著式编程：让计算机自己总结规律的编程方法。比如规定行为（如左转、后退等）和收益函">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记">
<meta property="og:url" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/index.html">
<meta property="og:site_name" content="备忘录">
<meta property="og:description" content="1 引言1.1 机器学习定义第一个定义——Arthur Samuel对机器学习的定义：  机器学习是这样的领域，它赋予计算机学习的能力，这种学习能力不是通过显著式编程获得的。   显著式编程：比如发一系列指令（左转-开门-右转……）让机器人到教室门外的咖啡机前冲咖啡，其劣势是需要把机器人所处的环境调查得一清二楚， 非显著式编程：让计算机自己总结规律的编程方法。比如规定行为（如左转、后退等）和收益函">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/常用K.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/PR.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/ROC.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/树状分类器.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/MP.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/两层P_1.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/单层P.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/两层P_2.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/三层网络.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/3例.png">
<meta property="og:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/3例_2.png">
<meta property="og:image" content="https://panchuang.net/wp-content/uploads/2019/11/1-1573310259.png">
<meta property="og:image" content="https://panchuang.net/wp-content/uploads/2019/11/9-1573310259.png">
<meta property="og:updated_time" content="2020-04-08T13:46:15.633Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习笔记">
<meta name="twitter:description" content="1 引言1.1 机器学习定义第一个定义——Arthur Samuel对机器学习的定义：  机器学习是这样的领域，它赋予计算机学习的能力，这种学习能力不是通过显著式编程获得的。   显著式编程：比如发一系列指令（左转-开门-右转……）让机器人到教室门外的咖啡机前冲咖啡，其劣势是需要把机器人所处的环境调查得一清二楚， 非显著式编程：让计算机自己总结规律的编程方法。比如规定行为（如左转、后退等）和收益函">
<meta name="twitter:image" content="http://sibyl233.github.io/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/常用K.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://sibyl233.github.io/2020/02/25/机器学习笔记/index.html">
  
    <link rel="alternate" href="/atom.xml" title="备忘录" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
  
  
</head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/cofess" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">昵称</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Web Developer &amp; Designer</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-books">
          <a href="/books">
            
            <i class="icon icon-book-fill"></i>
            
            <span class="menu-title">书单</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">友链</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/cofess" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://weibo.com/cofess" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/指南/">指南</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/笔记/">笔记</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/胡言/">胡言</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      

    
      
    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/笔记/">笔记</a>
              </p>
              <p class="item-title">
                <a href="/2020/02/25/机器学习笔记/" class="title">机器学习笔记</a>
              </p>
              <p class="item-date">
                <time datetime="2020-02-25T09:03:04.000Z" itemprop="datePublished">2020-02-25</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/指南/">指南</a>
              </p>
              <p class="item-title">
                <a href="/2020/02/20/Photoshop精简操作/" class="title">Photoshop精简操作</a>
              </p>
              <p class="item-date">
                <time datetime="2020-02-20T01:51:32.000Z" itemprop="datePublished">2020-02-20</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/指南/">指南</a>
              </p>
              <p class="item-title">
                <a href="/2020/01/08/Zotero简明使用指南/" class="title">Zotero简明使用指南</a>
              </p>
              <p class="item-date">
                <time datetime="2020-01-08T08:22:23.000Z" itemprop="datePublished">2020-01-08</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/笔记/">笔记</a>
              </p>
              <p class="item-title">
                <a href="/2019/09/23/Semantic-Segmentation/" class="title">语义分割方法整理</a>
              </p>
              <p class="item-date">
                <time datetime="2019-09-23T05:43:58.000Z" itemprop="datePublished">2019-09-23</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/笔记/">笔记</a>
              </p>
              <p class="item-title">
                <a href="/2019/07/11/道路场景语义分割/" class="title">道路场景语义分割</a>
              </p>
              <p class="item-date">
                <time datetime="2019-07-11T03:35:50.000Z" itemprop="datePublished">2019-07-11</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-机器学习笔记" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      机器学习笔记
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2020/02/25/机器学习笔记/" class="article-date">
	  <time datetime="2020-02-25T09:03:04.000Z" itemprop="datePublished">2020-02-25</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/笔记/">笔记</a>
  </span>

        

        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2020/02/25/机器学习笔记/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><h3 id="1-1-机器学习定义"><a href="#1-1-机器学习定义" class="headerlink" title="1.1 机器学习定义"></a>1.1 机器学习定义</h3><p>第一个定义——Arthur Samuel对机器学习的定义：</p>
<blockquote>
<p>机器学习是这样的领域，它赋予计算机学习的能力，这种学习能力不是通过显著式编程获得的。</p>
</blockquote>
<ul>
<li>显著式编程：比如发一系列指令（左转-开门-右转……）让机器人到教室门外的咖啡机前冲咖啡，其劣势是需要把机器人所处的环境调查得一清二楚，</li>
<li>非显著式编程：让计算机自己总结规律的编程方法。比如规定行为（如左转、后退等）和收益函数（如摔倒时收益函数为负值），让计算机自己去找最大化收益函数的行为模式。</li>
</ul>
<p>第二个定义——Tom Mitshell于1998年在他的书《Machine Learning》中的定义</p>
<blockquote>
<p>一个计算机程序被称为可以学习，是指它能够针对某个任务T和某个性能指标P，从经验E中学习。这种学习的特点是，它在T上的被P所衡量的性能，会随着经验E的增加而提高。</p>
</blockquote>
<ul>
<li>任务（Task, T）</li>
<li>经验（Experience, E）</li>
<li>性能指标（Performance Measure, P）</li>
</ul>
<h3 id="1-2-机器学习任务的分类"><a href="#1-2-机器学习任务的分类" class="headerlink" title="1.2 机器学习任务的分类"></a>1.2 机器学习任务的分类</h3><p>按照任务是否需要和环境交互获得经验分为<strong>监督学习</strong>和<strong>强化学习</strong>两类。比如在以下四个任务中，2和3属于监督学习，所有经验E都是人工采集（训练数据+标签）并输入计算机；1和4属于强化学习，经验E是由计算机与环境互动获得的。不过这样的划分并不绝对，比如在ALPHAGO中，首先通过监督学习（棋局）获得初始围棋程序，再通过强化学习提升棋力。</p>
<ol>
<li>教计算机下棋</li>
<li>垃圾邮件识别</li>
<li>人脸识别</li>
<li>无人驾驶</li>
</ol>
<p>对于<strong>监督学习</strong>可以进一步分类。</p>
<ul>
<li>按照训练数据是否存在标签分为<ul>
<li>传统监督学习<ul>
<li>支持向量机</li>
<li>人工神经网络</li>
<li>深度神经网络</li>
</ul>
</li>
<li>无监督学习<ul>
<li>聚类</li>
<li>EM算法</li>
<li>主成分分析</li>
</ul>
</li>
<li>半监督学习</li>
</ul>
</li>
<li>按照标签是连续还是离散分为<ul>
<li>分类</li>
<li>回归</li>
</ul>
</li>
</ul>
<h3 id="1-3-机器学习算法的过程"><a href="#1-3-机器学习算法的过程" class="headerlink" title="1.3 机器学习算法的过程"></a>1.3 机器学习算法的过程</h3><p>特征提取、特征选择 → 不同的算法对特征空间做不同的划分 → 不同的结果</p>
<h3 id="1-4-没有免费午餐定理"><a href="#1-4-没有免费午餐定理" class="headerlink" title="1.4 没有免费午餐定理"></a>1.4 没有免费午餐定理</h3><p>1995年，D.H.Wolpert等人提出没有免费午餐定理（No Free Lunch Theorem）：</p>
<blockquote>
<p>任何一个预测函数，如果在一些训练样本上表现好，那么必然在另一些训练样本上表现不好，如果不对数据在特征空间的先验分布有一定假设，那么表现好与表现不好的情况一样多。</p>
</blockquote>
<p>即如果不对特征空间的先验分布有假设，那么所有算法的表现都是一样的。再好的算法也有犯错的风险，因为机器学习的本质是通过有限的已知数据在复杂的高维特征空间中预测未知的样本，然而没有人知道未知样本在哪里，性质到底如何；也没有放之四海而皆准的最好算法，因为评价算法的好坏涉及到对特征空间先验分布的假设，然而没有人知道先验分布的真实样子。</p>
<p>在设计机器学习算法时的常用假设：</p>
<blockquote>
<p>在特征空间上距离接近的样本，他们属于同一个类别的概率会更高。</p>
</blockquote>
<p><div style="page-break-after: always;"></div></p>
<h2 id="2-支持向量机"><a href="#2-支持向量机" class="headerlink" title="2 支持向量机"></a>2 支持向量机</h2><h3 id="2-1-问题描述"><a href="#2-1-问题描述" class="headerlink" title="2.1 问题描述"></a>2.1 问题描述</h3><p>首先，明确线性可分和非线性可分的基础概念。</p>
<ul>
<li><p>直观定义：</p>
<ul>
<li>对于二维特征空间，存在一条直线将训练样本分开为线性可分，反之为非线性可分；</li>
<li>对于高维特征空间，存在一个超平面将训练样本分开为线性可分，反之为非线性可分。</li>
</ul>
</li>
<li><p>数学定义：</p>
</li>
</ul>
<p>假设我们有$N$个训练样本和它们的标签：</p>
<script type="math/tex; mode=display">
\begin{aligned}
  &\left\{\left(X_{1}, y_{1}\right),\left(X_{2}, y_{2}\right), \ldots,\left(X_{N}, y_{N}\right)\right\}\\
  &X_{i}=\left[x_{i 1}, x_{i 2}\right]^{T}\\
  &y_{i}=\{+1,-1\}
  \end{aligned}</script><p>线性可分是指存在 $(\omega,b)$ 使得对 $i=1\sim N$ 有</p>
<script type="math/tex; mode=display">
y_{i}\left(\omega^{T} X_{i}+b\right)>0</script><p>1995年，Vladimir Vapnik提出支持向量机寻找的最优分类直线应满足以下3个条件。</p>
<ol>
<li>该直线分开了两类</li>
<li>该直线最大化间隔（margin）</li>
<li>该直线处于间隔的中间，到所有支持向量的距离相等</li>
</ol>
<p>此时我们把样本点中与分离超平面距离最近的数据点称为<strong>支持向量</strong>。在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用。如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响。</p>
<h3 id="2-2-线性可分-硬间隔"><a href="#2-2-线性可分-硬间隔" class="headerlink" title="2.2 线性可分-硬间隔"></a>2.2 线性可分-硬间隔</h3><p>假定训练样本是线性可分的，该优化问题可以写成如下形式：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\text {Minimize: }&& \frac{1}{2}\|\omega\|^{2} \tag{2.2.1}\\
\text{s.t.: }&& {y}_{i}\left(\omega^{T} x_{i}+b\right) \geq 1,(i=1 \sim N) \tag{2.2.2}
\end{eqnarray*}</script><p>其中待求的是 $\omega$ 和 $b$ ，该形式的推导过程如下。</p>
<p><strong>[事实1]</strong> $(\omega,b)$ 表示的超平面和 $(a\omega,ab)$ 表示的超平面是同一个平面。所以我们可以用参数 $a$ 来缩放 $(\omega,b)$，使它满足下式。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\left|\omega^{T} x_{0}+b\right|=1 \quad \text{on support vector}\ x_{0}\\
&\left|\omega^{T} x_{0}+b\right|>1 \quad \text{on non-support vectors}
\end{aligned}</script><p>限制条件 (2.2.2) 中 $y_{i}$ 可以协调超平面的左右，使得对于非支持向量分布于超平面的两侧。</p>
<p><strong>[事实2]</strong> 支持向量到超平面的距离公式众所周知。结合事实1可得到下式，所以要最大化支持向量到超平面的距离就等价于最小化 $|\omega|$。</p>
<script type="math/tex; mode=display">
d=\frac{\left|\omega^{T} x_{0}+b\right|}{\|\omega\|}=\frac{1}{\|\omega\|}</script><p> 最后目标函数的形式如 (2.2.1) 所示，主要是为了求导的方便。</p>
<h3 id="2-3-非线性可分-软间隔"><a href="#2-3-非线性可分-软间隔" class="headerlink" title="2.3 非线性可分-软间隔"></a>2.3 非线性可分-软间隔</h3><p>在线性不可分情况下，不存在 $(\omega,b)$ 满足如 (2.2.2) 的限制条件，所以需要适当放松限制条件，引入松弛变量 $\delta<em>{i}$ ，同时也要限制 $\delta</em>{i}$ 的范围避免其无限大。改造后的优化问题如下所示：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\text {Minimize: }&& \frac{1}{2}\|\omega\|^{2}+C \sum_{i=1}^{N} \delta_{i}^{2}\\
\text{s.t.: }&&(1)\ \delta_{i} \geq 0,(i=1 \sim N)\\
&&(2)\ y_{i}\left(\omega^{T} X_{i}+b\right) \geq 1-\delta_{i},(i=1 \sim N)
\end{eqnarray*}</script><p>其中 $(\omega,b,\delta_{i})$ 为待求项，比例因子 $C$ 作为<strong>超参数 (Hyper Parameter)</strong> 需要人为设定，起到了平衡两项的作用。</p>
<h3 id="2-4-非线性可分-核技巧"><a href="#2-4-非线性可分-核技巧" class="headerlink" title="2.4 非线性可分-核技巧"></a>2.4 非线性可分-核技巧</h3><p>软间隔的方式作用有限，对于非线性可分数据集有时需要曲面，所以只能扩大可选函数的范围使其超越线性。基本思路是将训练样本的特征空间从低维映射到高维，再用线性超平面对数据进行分类。以异或问题为例：</p>
<p>我们不妨先假设这个映射函数为 $\phi(x)$ ，那么优化问题将变为如下形式：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\text {Minimize: }&& \frac{1}{2}\|\omega\|^{2}+C \sum_{i=1}^{N} \delta_{i}^{2}\\
\text{s.t.: }&&(1)\ \delta_{i} \geq 0,(i=1 \sim N)\\
&&(2)\ y_{i}\left[\omega^{T} \varphi(X_{i})+b\right] \geq 1-\delta_{i},(i=1 \sim N)
\end{eqnarray*}</script><p>求解上述优化问题的对偶问题涉及到计算 $\varphi\left(X<em>{1}\right)^{T} \varphi\left(X</em>{2}\right)$。由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算 $\varphi\left(X<em>{1}\right)^{T}\varphi\left(X</em>{2}\right)$ 通常是困难的，为了避开这个障碍，我们引入核函数 $K$。后续章节将说明我们不需要显式地定义映射 $\varphi $ 是什么，而只需事先定义核函数 $K$ ，就可以利用解线性问题的方法求解非线性问题的支持向量机。本节首先对核函数 $K$ 有个初步的了解。</p>
<ul>
<li><p>核函数的定义：$K\left(X<em>{1}, X</em>{2}\right)=\varphi\left(X<em>{1}\right)^{T} \varphi\left(X</em>{2}\right)$</p>
</li>
<li><p>核函数的充要条件：①交换性；②半正定性 。<strong>（Mercer定理）</strong></p>
</li>
<li>核函数 $K$ 和映射函数 $\phi $ 一一对应。</li>
<li>常用核函数：</li>
</ul>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/常用K.png" alt="常用K" style="zoom: 50%;"></p>
<h3 id="2-5-原问题和对偶问题"><a href="#2-5-原问题和对偶问题" class="headerlink" title="2.5 原问题和对偶问题"></a>2.5 原问题和对偶问题</h3><ul>
<li>原问题（Prime Problem）</li>
</ul>
<p>其中自变量 $\omega$ 为多维向量，$f(\omega)$为目标函数，$g<em>{i}(\omega)$ 和 $h</em>{i}(\omega)$为限制条件。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {Minimize: }& f(\omega) \\
\text{s.t.: }& g_{i}(\omega) \leq 0 \quad i=1 \sim K \\
&h_{i}(\omega)=0 \quad i=1 \sim m
\end{aligned}</script><ul>
<li>对偶问题（Dual Problem）</li>
</ul>
<p>首先定义函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\omega, \alpha, \beta) &=f(\omega)+\sum_{i=1}^{K} \alpha_{i} g_{i}(\omega)+\sum_{i=1}^{M} \beta_{i} h_{i}(\omega) \\
&=f(\omega)+\alpha^{T} g(\omega)+\beta^{T} h(\omega)
\end{aligned}</script><p>在此基础上定义对偶问题如下,其中 $\theta(\alpha, \beta)$ 表示在遍历所有的 $\omega$ 后取 $L(\omega, \alpha, \beta)$ 的最小值</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Maxmize: }& \theta(\alpha, \beta)=\inf_{\omega} \ L(\omega, \alpha, \beta) \\
\text{s.t.: }& \alpha_{i} \geq 0, i=1 \sim K
\end{aligned}</script><ul>
<li>原问题与对偶问题的关系</li>
</ul>
<p><strong>[定理1] 弱对偶性</strong>：如果 $\omega^<em>$ 是原问题的解，$\alpha^</em>$ 和 $\beta^*$  是对偶问题的解，那么</p>
<script type="math/tex; mode=display">
f\left(\omega^{*}\right) \geqslant \theta\left(\alpha^{*}, \beta^{*}\right)</script><p>证明：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta\left(\alpha^{*}, \beta^{*}\right) &=\inf L\left(\omega, \alpha^{*}, \beta^{*}\right) \\
& \leq L\left(w^{*}, \alpha^{*}, \beta^{*}\right) \\
&=f\left(\omega^{*}\right)+\alpha^{* T} g\left(\omega^{*}\right)+\beta^{* T} h\left(\omega^{*}\right) \\
& \leq f\left(\omega^{*}\right)
\end{aligned}</script><p><strong>[定理2] 强对偶性</strong>：如果 $g(\omega)=A\omega+b$，$h(\omega)=C\omega+d$，$f(\omega)$ 为凸函数，那么</p>
<script type="math/tex; mode=display">
f\left(\omega^{*}\right)=\theta\left(\alpha^{*}, \beta^{*}\right)</script><p>证明略，见附录1。</p>
<p><strong>[定义1] Slater条件</strong>：即强对偶性成立的条件。</p>
<p><strong>[定义2] KKT条件</strong>：对所有 $i=1\sim K$ 要么 $\alpha_{i}^{<em>}=0$， 要么 $g(\omega^{</em>})=0$</p>
<p>推导：由定理1证明部分的第3行到第4行易知，要取等号必须满足KKT条件。</p>
<p>注释：KKT是三人名字的首字母。</p>
<h3 id="2-6-SVM转化为对偶问题"><a href="#2-6-SVM转化为对偶问题" class="headerlink" title="2.6 SVM转化为对偶问题"></a>2.6 SVM转化为对偶问题</h3><h3 id="2-7-算法总体流程"><a href="#2-7-算法总体流程" class="headerlink" title="2.7 算法总体流程"></a>2.7 算法总体流程</h3><p>接着2.6节，总结支持向量机训练和测试的流程。</p>
<ul>
<li>训练过程</li>
</ul>
<p>（1）输入训练数据</p>
<script type="math/tex; mode=display">
\left\{\left(X_{i}, y_{i}\right)\right\},\text{where }y_{i}=\{+1,-1\}</script><p>（2）用<u>SMO算法</u>求解如下优化问题，解出 $\alpha_{i}$</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\text{Maxmize: }&& \theta(\alpha, \beta)=\sum_{i=1}^{N} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} y_{i} y_{j} \alpha_{i} \alpha_{j} \varphi\left(X_{i}\right)^{\mathrm{T}} \varphi\left(X_{j}\right) \tag{2.7.1}\\
\text{s.t.: }&& (1)\ 0\leq \alpha_{i} \leq C, \ i=1 \sim N \\
&&(2)\ \sum_{i=1}^{N} \alpha_{i} y_{i}=0, \ i=1 \sim N
\end{eqnarray*}</script><p>（3）找一个 $\alpha<em>{i}\neq 0$ 且 $\alpha</em>{i}\neq C$ ，利用下式求 $b$</p>
<script type="math/tex; mode=display">
b=\frac{1-\sum_{j=1}^{N} \alpha_{j} y_{i} y_{j} K\left(X_{j}, X_{i}\right)}{y_{i}} \tag{2.7.2}</script><ul>
<li><p>测试过程</p>
<p>考察测试数据 $X$，预测其类别 $y$</p>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&\text{if }\sum_{i=1}^{N} \alpha_{i} y_{i} K\left(X_{i}, X\right)+b \geq 0, \quad \text{then } y=+1\\
&\text{if }\sum_{i=1}^{N} \alpha_{i} y_{i} K\left(X_{i}, X\right)+b<0, \quad \text{then } y=-1
\end{aligned}</script><p><strong>[推导]</strong> 在通过解凸优化问题得到 $\alpha_{i}$ 后，求偏移项 $b$ 的过程如下</p>
<script type="math/tex; mode=display">
\omega=\sum_{j=1}^{N} \alpha_{j} y_{j} \varphi\left(X_{i}\right)</script><script type="math/tex; mode=display">
\begin{aligned}\omega^{T} \varphi\left(x_{i}\right) &=\sum_{j=1}^{N} \alpha_{j} y_{j} \varphi\left(X_{j}\right)^{T} \varphi\left(X_{i}\right) \\&=\sum_{j=1}^{N} \alpha_{j} y_{j} K\left(X_{j}, X_{i}\right)\end{aligned}</script><p>根据KKT条件有</p>
<script type="math/tex; mode=display">
\begin{aligned}&\alpha_{i}\left[1+\delta_{i}-y_{i} \omega^{T} \varphi\left(X_{i}\right)-y_{i} b\right]=0 \\&\beta_{i} \delta_{i}=0 \Rightarrow \left(c-\alpha_{i}\right) \delta_{i}=0\end{aligned}</script><p>如果对某个 $i$，$\alpha<em>{i}\neq 0$ 且 $\alpha</em>{i}\neq C$ 那么必有</p>
<script type="math/tex; mode=display">
\delta_{i}=0 \\1+\delta_{i}-y_{i} \omega^{T} \varphi\left(X_{i}\right)-y_{i} b=0</script><p>从而得到式 (2.7.2)</p>
<script type="math/tex; mode=display">
b=\frac{1-\sum_{j=1}^{N} \alpha_{j} y_{i} y_{j} K\left(X_{j}, X_{i}\right)}{y_{i}}</script><p><strong>[注释] SMO算法</strong>：SMO 的基本思路是先固定 $\alpha<em>{i}$ 之外的所有参数，然后求 $\alpha</em>{i}$ 的极值。由于存在约束 $\sum<em>{i=1}^{N} \alpha</em>{i} y<em>{i}=0$，若固定 $\alpha</em>{i}$ 之外的其他变量，则 $\alpha<em>{i}$ 可由其他变量导出。于是， SMO 每次选择两个变量 $\alpha</em>{i}$ 和 $\alpha_{j}$ ，并固定其他参数。这样，在参数初始化后， SMO 不断执行如下两个步骤直至收敛：</p>
<ul>
<li>选取一对需更新的变量 $\alpha<em>{i}$ 和 $\alpha</em>{j}$；</li>
<li>固定 $\alpha<em>{i}$ 和 $\alpha</em>{j}$ 以外的参数，求解式 (2.7.1) 获得更新后的 $\alpha<em>{i}$ 和 $\alpha</em>{j}$。</li>
</ul>
<p><strong>[概念] 二次规划问题</strong>：二次规划 (Quadratic Programming，简称 QP)是一类典型的优化问题，包括凸二次优化和非凸二次优化。在此类问题中，<u>目标函数是变量的二次函数，而约束条件是变量的线性不等式</u>。假定变量个数为 $d$， 约束条件的个数为 $m$，则标准的二次规划问题形如</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{\boldsymbol{x}} \frac{1}{2} \boldsymbol{x}^{\mathrm{T}} \mathbf{Q} \boldsymbol{x}+\boldsymbol{c}^{\mathrm{T}} \boldsymbol{x}\\
&\text { s.t. } \quad \mathbf{A} \boldsymbol{x} \leqslant \boldsymbol{b}
\end{aligned}</script><p>若 $\mathbf{Q}$ 为半正定矩阵，则目标函数是凸函数，相应的二次规划是凸二次优化问题，此时该问题要么无解要么有全局最小值；若 $\mathbf{Q}$ 为正定矩阵，则该问题有唯一的全局最小值；若 $\mathbf{Q}$ 为非正定矩阵，则上式是有多个平稳点和局部极小点的 NP 难问题。</p>
<h3 id="2-8-兵王问题"><a href="#2-8-兵王问题" class="headerlink" title="2.8 兵王问题"></a>2.8 兵王问题</h3><h3 id="2-9-性能度量"><a href="#2-9-性能度量" class="headerlink" title="2.9 性能度量"></a>2.9 性能度量</h3><p>首先，介绍混淆矩阵如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实际\预测</th>
<th>正样本</th>
<th>负样本</th>
</tr>
</thead>
<tbody>
<tr>
<td>正样本</td>
<td>TP (True Positive)</td>
<td>FN (False Negative)</td>
</tr>
<tr>
<td>负样本</td>
<td>FP (False Positive)</td>
<td>TN (True Negative)</td>
</tr>
</tbody>
</table>
</div>
<p>其次，介绍 5 个概率指标：①查准率P；②查全率/召回率R；③真正样本率TPR ；④假正样本率FPR；⑤准确率ACC。</p>
<script type="math/tex; mode=display">
\begin{aligned}
P &=\frac{TP}{TP+FP} \\
R &=\frac{TP}{TP+FN} \\
TPR &=\frac{TP}{TP+FN}\\
FPR &=\frac{FP}{TN+FP} \\
ACC &= \frac{TP+TN}{TP+FN+FP+TN}
\end{aligned}</script><p>接着，明确 3 个关系：①TPR+FNR=1；②FPR+TNR=1；③对同一个系统来说，若TPR增加，则FPR也增加。</p>
<p>对于第③个关系的直观理解是：如果我们把更多的正样本识别为正样本，则我们一定会把更多的负样本识别为负样本。</p>
<ul>
<li><strong>P-R曲线</strong>：横坐标为R，纵坐标为P</li>
</ul>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/PR.png" alt="PR" style="zoom: 40%;"></p>
<ul>
<li><strong>ROC曲线</strong>：横坐标为FPR，纵坐标为TPR<ul>
<li>AUC：也就是ROC曲线下的面积，是一个0到1之间的数，AUC越大说明系统性能越好。</li>
<li>EER ：也就是FPR=FNR的值，由于FNR=1-TPR，可以画一条从 (0,1) 到 (1,0) 的直线来找到交点。ERR越小系统性能越好。</li>
</ul>
</li>
</ul>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/ROC.png" alt="ROC" style="zoom: 33%;"></p>
<h3 id="2-10-SVM求解多分类问题"><a href="#2-10-SVM求解多分类问题" class="headerlink" title="2.10 SVM求解多分类问题"></a>2.10 SVM求解多分类问题</h3><p>二分类的多支持向量机求解多分类问题（假设共有K类），有如下2种策略：</p>
<ol>
<li>1类对K-1类：需要构造K个支持向量机，存在训练样本不平衡的问题。</li>
<li>1类对另1类：需要构造K(K-1)/2个支持向量机，存在训练和测试时间过长的问题。</li>
</ol>
<p>所以往往综合上述 2 种策略，构造树形分类器。如下图所示，8类只需要构造7个支持向量机，同时兼顾了样本数的平衡。值得一提的是，我们需要保证每个分类器区分的两类的差别是显著的，对于这一点我们可以利用<strong>聚类算法/决策树算法</strong>来解决。</p>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/树状分类器.png" alt="树状分类器" style="zoom:33%;"></p>
<p><div style="page-break-after: always;"></div></p>
<h2 id="3-人工神经网络"><a href="#3-人工神经网络" class="headerlink" title="3 人工神经网络"></a>3 人工神经网络</h2><h3 id="3-1-神经元的数学模型"><a href="#3-1-神经元的数学模型" class="headerlink" title="3.1 神经元的数学模型"></a>3.1 神经元的数学模型</h3><p>人工智能的 2 个学派：</p>
<ul>
<li>仿生学派：如人工神经网络</li>
<li>数理学派：如支持向量机</li>
</ul>
<p>在生物神经网络中，每个神经元与其他神经元相连，当它”兴奋”时， 就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了 一个”阈值” ，那么它就会被激活，即 “兴奋 “起来，向其他神经元发送化学物质。</p>
<p>1943年，心理学家W.S.McCulloch和数理逻辑学家W.Pitts将上述情形抽象为图所示的简单模型，这就是一直沿用至今 的 “M-P神经元模型 “ 在这个模型中 ，神经元接收到来自 $n$ 个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阀值进行比较，然后通过”激活函数” 处理以产生神经元的输出。</p>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/MP.png" alt="MP" style="zoom: 40%;"></p>
<p>[注释1]：为什么采用加权求和加上偏置的形式？如果假设神经元的输出 $y$ 是输入 $x_{i}$ 的函数 $f$，那么 $f$ 的一阶泰勒展开就是这样的形式</p>
<script type="math/tex; mode=display">
\begin{aligned}
y &=f\left(x_{1}, x_{2}, \cdots x_{n}\right) \\
& \approx f(0,0, \cdots 0)+\sum_{i=1}^{n}\left[\frac{\partial f}{\partial x_{i}} |(0,0, \cdots 0)\right] x_{i}+\ldots \\
&=\sum_{i=1}^{m} \omega_{i} x_{i}+b
\end{aligned}</script><h3 id="3-2-感知器算法及其意义"><a href="#3-2-感知器算法及其意义" class="headerlink" title="3.2 感知器算法及其意义"></a>3.2 感知器算法及其意义</h3><p><strong>感知器 (Perceptron)</strong> 由两层神经元组成， 输入层接收外界输入信号，输出层是M-P神经元。其数学表达式为：</p>
<script type="math/tex; mode=display">
Y=\varphi(W^TX+b)</script><p><strong>感知器算法</strong>从 $(X,y )$ 对中通过学习获得 $W$ 和 $b$，步骤如下：</p>
<ol>
<li>随机选择 $W$ 和 $b$</li>
<li>取一个训练样本 $(X,y )$ <ol>
<li>若 $W^{T}X+b&gt;0$，且 $y=-1$ 则：$W=W-X,\ b=b-1$</li>
<li>若 $W^{T}X+b&lt;0$，且 $y=+1$ 则：$W=W+X,\ b=b+1$</li>
</ol>
</li>
<li>再取一个训练样本 $(X,y )$ 回到步骤 2</li>
<li>终止条件：直到所有输入输出对都不满足步骤 2 中的 2 个情形之一，退出循环</li>
</ol>
<p><strong>[证明1]</strong> 步骤 2 的合理性：以情形 1 为例，可以看到新值比原来至少减少了1，从而更加接近平衡</p>
<script type="math/tex; mode=display">
\begin{aligned}
&W_{new}^{\mathrm{T}} X+b_{new} \\
=&(W-X)s^{T} X+b-1 \\
=&\left(W^{T} X+b\right)-\left(X^{T} X+1\right). \\
=&\left(W^{T} X+b\right)-\left(\|X\|^{2}+1\right). \\
\leq&\left(W^{T} X+b\right)-1.
\end{aligned}</script><p><strong>[证明2]</strong> 步骤 4 的合理性：即证明感知器算法的收敛定理，过程与证明1类似，详见讲义4。将<strong>收敛定理</strong>用增广向量的形式进行描述：对于 $N$ 个增广向量，如果存在一个权重向量 $\omega<em>{opt}$ 使得对于每一个 $i$ 有  $\omega</em>{opt}^{T}x<em>{i}&gt;0$ ，运用上述感知器算法，必能够在有限步内，找到一个 $\omega$ 使得对所有 $i$ 有 $\omega^{T}x</em>{i}&gt;0$ 。该定理有 2 个注意点：①前提条件：训练集线性可分 (存在一个 $\omega<em>{opt}$ 使得 $\omega</em>{opt}^{T}x<em>{i}&gt;0$ )，所以感知器算法只能解决线性可分问题；②在有限步内找到的 $\omega$ 未必是 $\omega</em>{opt}$，所以其效果不如支持向量机。</p>
<p>感知器算法具有重要的历史意义：</p>
<ul>
<li>提出了一套机器学习算法的框架：运用训练数据集 $(X_i,y_i)$ 求出待学习的参数 $\theta$，来寻找预测函数 $y=f(X;\theta)$</li>
</ul>
<script type="math/tex; mode=display">
X \rightarrow f(X;\theta) \rightarrow Y</script><ul>
<li>相较于支持向量机这样的全局优化问题，消耗的计算资源和内存资源较少，是此类算法的先驱。</li>
</ul>
<h3 id="3-3-多层神经网络"><a href="#3-3-多层神经网络" class="headerlink" title="3.3 多层神经网络"></a>3.3 多层神经网络</h3><p>单层感知器的学习过程只在训练集线性可分的前提下收敛，若要解决非线性可分问题，就需考虑多层感知器/多层神经网络。比如两层感知器就可以解决非线性的异或问题：</p>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/两层P_1.png" alt="两层P_1" style="zoom:33%;"></p>
<p>花点时间明确一下概念</p>
<ul>
<li>单层感知器：输入层+输出层</li>
</ul>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/单层P.png" alt="单层P" style="zoom: 33%;"></p>
<ul>
<li>两层感知器/两层网络/单隐层网络：输入层+隐层+输出层</li>
</ul>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/两层P_2.png" alt="两层P_2" style="zoom:33%;"></p>
<ul>
<li>三层网络：输入层+隐层1+隐层2+输出层</li>
</ul>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/三层网络.png" alt="三层网络" style="zoom:33%;"></p>
<p><strong>[注释1]</strong> 为什么两层网络看起来有三层？因为输入层神经元仅仅是接受输入，不进行函数处理，所以通常称之为两层网络，为了避免歧义有时也称之为单隐层网络。</p>
<p><strong>[注释2]</strong> 隐层神经元必须是功能神经元的必要性：不难证明，如果不加非线性函数，多层神经网络将会退化到一个神经元的M-P模型状态。</p>
<p><strong>[注释3]</strong> 如果层与层之前的非线性函数是阶跃函数，理论上三层神经网络可以模拟任意的非线性函数。</p>
<h3 id="3-4-梯度下降法"><a href="#3-4-梯度下降法" class="headerlink" title="3.4 梯度下降法"></a>3.4 梯度下降法</h3><p>为了使得神经网络训练的输出 $y$ 和标签 $Y$ 尽可能接近，将优化问题表示如下</p>
<script type="math/tex; mode=display">
Minimize:E(\omega,b)=E_{(X,Y)}[(Y-y)^2]</script><p>其中 $E_{(X,Y)}$ 是遍历训练样本及标签的数学期望。由于 $y$ 是 $(W,b)$ 的非凸函数，因此无法求到唯一的全局极值，而是用<strong>梯度下降法</strong>求解目标函数的局部极小值。</p>
<ol>
<li>随机选取 $\omega$ 和 $b$ 的初始值 $(\omega^{(0)},b^{(0)})$</li>
<li>应用迭代算法求目标函数的局部极值，其中 $\alpha$ 即<strong>学习率</strong>。</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
&\omega^{(n+1)}=\omega^{(n)}-\left.\alpha \frac{\partial E}{\partial \omega}\right|_{\omega^{(n)}, b^{(n)}}\\
&b^{(n+1)}=b^{(n)}-\left.\alpha \frac{\partial E}{\partial b}\right|_{\omega^{(n)}, b^{(n)}}
\end{aligned}</script><p>如果把梯度下降法应用到一个具体的例子中。</p>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/3例.png" alt="3例" style="zoom:50%;"></p>
<p>假设目标函数为：</p>
<script type="math/tex; mode=display">
Minimize:E(\omega,b)=\frac{1}{2}(Y-y)^2</script><p>那么待估计的参数有：</p>
<script type="math/tex; mode=display">
(w_{11}, w_{12}, w_{21}, w_{22}, w_{1}, w_{2}, b_{1}, b_{2}, b_{3})</script><p>梯度下降法相应就要求九个偏导数：</p>
<script type="math/tex; mode=display">
\left(\frac{\partial E}{\partial \omega_{11}}, \frac{\partial E}{\partial \omega_{12}}, \frac{\partial E}{\partial \omega_{21}}, \frac{\partial E}{\partial \omega_{22}}, \frac{\partial E}{\partial \omega_{1}}, \frac{\partial E}{\partial \omega_{2}}, \frac{\partial E}{\partial b_{1}}, \frac{\partial E}{\partial b_{2}}, \frac{\partial E}{\partial b_{3}}\right)</script><p>下一节内容讲的就是如何通过神经网络这种分层结构简化偏导数的计算过程。</p>
<h3 id="3-5-后向传播算法"><a href="#3-5-后向传播算法" class="headerlink" title="3.5 后向传播算法"></a>3.5 后向传播算法</h3><p><strong>后向传播算法框架</strong></p>
<ol>
<li>对神经网络每一层的各个神经元，随机选取相应的 $\omega$ 和 $b$ </li>
<li>前向计算，对于输入的训练数据，计算并保留每一层的输出值，直到计算出最后一层的输出 $y$</li>
<li>设置目标函数 $E$，用后向传播算法对每一个 $\omega$ 和 $b$ 计算 $\frac{\partial E}{\partial \omega}, \frac{\partial E}{\partial b}$</li>
<li>利用梯度下降法，更新 $\omega$ 和 $b$ 的值</li>
<li>回到第2步，不断循环，直到所有 $\left.\frac{\partial E}{\partial \omega}\right|<em>{\omega^{(n)}, b^{(n)}}$，$\left.\frac{\partial E}{\partial b}\right|</em>{\omega^{(n)}, b^{(n)}}$ 很小为止。退出循环。</li>
</ol>
<p><strong>后向传播算法核心</strong></p>
<p>偏导数之间是相互关联的，根据链式求导法则，可以利用已经算出的偏导数求解其它偏导数。之所以称之为“后向”是因为这是一个从输出往输入推的过程，即先算出离输出较近的偏导数，再计算离输出较远的偏导数。</p>
<p>先考虑一个简单的例子，对应上一讲，将网络写成数学表达式。</p>
<p><img src="/2020/02/25/机器学习笔记/sibyl/Personal/blog/source/_posts/2020-02-25-机器学习笔记/3例_2.png" alt="3例_2" style="zoom:45%;"></p>
<script type="math/tex; mode=display">
\begin{array}{l}a_{1}=w_{11} x_{1}+w_{12} x_{2}+b_{1} \\a_{2}=w_{21} x_{1}+w_{22} x_{2}+b_{2} \\z_{1}=\varphi\left(a_{1}\right) \\z_{2}=\varphi\left(a_{2}\right) \\y=w_{1} z_{1}+w_{2} z_{2}+b_{3}\end{array}</script><p>首先求解如图标记的3个偏导数：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
&&\frac{\partial E}{\partial y}=y-Y \\
&&\frac{\partial E}{\partial a_{1}}=\frac{\partial E}{\partial y} \frac{\partial y}{\partial a_{1}}=\frac{\partial E}{\partial y} \frac{\partial y}{\partial z_{1}} \frac{d z_{1}}{d a_{1}}=\omega_{1}(y-Y) \varphi^{\prime}\left(a_{1}\right) \\
&&\frac{\partial E}{\partial a_{2}}=\frac{\partial E}{\partial y} \frac{\partial y}{\partial a_{2}}=\frac{\partial E}{\partial y} \frac{\partial y}{\partial z_{2}} \frac{d z_{2}}{d a_{2}}=\omega_{2}(y-Y) \varphi^{\prime}\left(a_{2}\right)
\end{eqnarray*}</script><p>求得上述枢纽函数的偏导数之后，由 $y=w<em>{1} z</em>{1}+w<em>{2} z</em>{2}+b_{3}$ 易得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial E}{\partial \omega_{1}}=\frac{\partial E}{\partial y} \frac{\partial y}{\partial \omega_{1}}=(y-Y) Z_{1}\\
&\frac{\partial E}{\partial \omega_{2}}=\frac{\partial E}{\partial y} \frac{\partial y}{\partial \omega_{2}}=(y-Y) Z_{2}\\
&\frac{\partial E}{\partial b_{3}}=\frac{\partial E}{\partial y} \frac{\partial y}{\partial b_{3}}=y-Y
\end{aligned}</script><p>由 $a<em>{1}=w</em>{11} x<em>{1}+w</em>{12} x<em>{2}+b</em>{1}$ 易得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial E}{\partial \omega_{11}}=\frac{\partial E}{\partial a_{1}} \frac{\partial a_{1}}{\partial \omega_{11}}=\omega_{1}(y-Y) \varphi^{\prime}\left(a_{1}\right) x_{1} \\ 
&\frac{\partial E}{\partial \omega_{12}}=\frac{\partial E}{\partial a_{1}} \frac{\partial a_{1}}{\partial \omega_{12}}=\omega_{1}(y-Y) \varphi^{\prime}\left(a_{1}\right) x_{2} \\ 
&\frac{\partial E}{\partial b_{1}}=\frac{\partial E}{\partial a_{1}} \frac{\partial a_{1}}{\partial b_{1}}=\omega_{1}(y-Y) \varphi^{\prime}\left(a_{1}\right)
\end{aligned}</script><p> 由 $a<em>{2}=w</em>{21} x<em>{1}+w</em>{22} x<em>{2}+b</em>{2}$ 易得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial E}{\partial \omega_{21}}=\frac{\partial E}{\partial a_{2}} \frac{\partial a_{2}}{\partial \omega_{21}}=\omega_{2}(y-Y) \varphi^{\prime}\left(a_{2}\right) x_{1} \\ 
&\frac{\partial E}{\partial \omega_{22}}=\frac{\partial E}{\partial a_{2}} \frac{\partial a_{2}}{\partial \omega_{22}}=\omega_{2}(y-Y) \varphi^{\prime}\left(a_{2}\right) x_{2} \\ 
&\frac{\partial E}{\partial b_{2}}=\frac{\partial E}{\partial a_{2}} \frac{\partial a_{2}}{\partial b_{2}}=\omega_{2}(y-Y) \varphi^{\prime}\left(a_{2}\right)
\end{aligned}</script><p>至此求得了九个偏导数。</p>
<p>接下来，我们考虑更一般的情况。将神经网络写成矩阵形式的如下表示式。其中 ①网络共有 $l$ 层；② $z^{(k)},a^{(k)},b^{(k)}$ 为向量，用 $z<em>{i}^{(k)},a</em>{i}^{(k)},b_{i}^{(k)}$ 表示其第 $i$ 个分量；③输出 $y$ 可以是向量，用 $y_i$ 表示其第 $i$ 个分量。</p>
<script type="math/tex; mode=display">
\begin{array}{l}x=a^{(0)} \Rightarrow z^{(1)}=w^{(1)} a^{(0)}+b^{(1)} \Rightarrow a^{(1)}=\varphi\left(z^{(1)}\right) \\ \quad \Rightarrow z^{(2)}=w^{(2)} a^{(1)}+b^{(2)} \Rightarrow a^{(2)}=\varphi\left(z^{(2)}\right) \cdots \\ \cdots \Rightarrow z^{(m)}=w^{(m)} a^{(m-1)}+b^{(m)} \Rightarrow a^{(m)}=\varphi\left(z^{(m)}\right) \cdots \\ \quad \cdots \Rightarrow z^{(l)}=w^{(l)} a^{(l-1)}+b^{(l)} \Rightarrow y=a^{(l)}=\varphi\left(z^{(l)}\right)\end{array}</script><p>同样假设目标函数为：</p>
<script type="math/tex; mode=display">
Minimize:E(\omega,b)=\frac{1}{2}(Y-y)^2</script><p>设置枢纽变量为：</p>
<script type="math/tex; mode=display">
\delta_{i}^{(\mathrm{m})}=\frac{\partial E}{\partial z_{i}^{(m)}}</script><p>最后一层为：</p>
<script type="math/tex; mode=display">
\delta_{i}^{(l)}=\frac{\partial E}{\partial z_{i}^{(l)}}=\frac{\partial E}{\partial y_{i}} \cdot \frac{\partial y_{i}}{\partial z_{i}^{(l)}}=\left(y_{i}-Y_{i}\right) \varphi^{\prime}\left(z_{i}^{(l)}\right)</script><p>通过第 $m+1$ 层推导到第 $m$ 层：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*} \delta_{i}^{(m)}=\frac{\partial E}{\partial z_{i}^{(m)}} &&=\sum_{j=1}^{S_{m+1}} \frac{\partial E}{\partial z_{j}^{(m+1)}} \cdot \frac{\partial z_{j}^{(m+1)}}{\partial z_{i}^{(m)}} \\ 
&&=\sum_{j=1}^{S_{m+1}} \delta_{j}^{(m+1)} \frac{\partial z_{j}^{(m+1)}}{\partial z_{i}^{(m)}} \\
&&=\left[\sum_{j=1}^{S_{m+1}} \delta_{j}^{(m+1)} W_{j i}^{(m+1)}\right] \varphi^{\prime}\left(Z_{i}^{(m)}\right) \tag{3.5.1}
\end{eqnarray*}</script><p>其中第2行到第3行的推导：</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial z_{j}^{(m+1)}}{\partial z_{i}^{(m)}} &=\frac{\partial z_{j}^{(m+1)}}{\partial \mathbf{a}_{i}^{(m)}} \frac{\partial a_{j}^{(m)}}{\partial z_{i}^{(m)}} \\ &=W_{j i}^{(m+1)} \varphi^{\prime}\left(Z_{i}^{(m)}\right) \end{aligned}</script><p>式（3.5.1）表明我们可以通过 $\delta<em>{i}^{(l)}$ 逐层向前递推 $\delta</em>{i}^{(m)}$，也易得：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
&&\frac{\partial E}{\partial W_{j i}^{(m)}}=\delta_{j}^{(m)} \cdot a_{i}^{(m-1)} \tag{3.5.2}\\ 
&&\frac{\partial E}{\partial b_{i}^{(m)}}=\delta_{i}^{(m)}\tag{3.5.3}
\end{eqnarray*}</script><h3 id="3-6-三个改进"><a href="#3-6-三个改进" class="headerlink" title="3.6 三个改进"></a>3.6 三个改进</h3><ul>
<li>非线性函数的改进</li>
</ul>
<p>问题：如果层与层之间的函数为阶跃函数，那么求导就会出现问题。</p>
<p>改进：常用的非线性函数有</p>
<ul>
<li>目标函数的改进</li>
</ul>
<p>问题：</p>
<p>改进：常采用基于softmax和cross-entropy的目标函数。</p>
<ul>
<li>参数更新的改进</li>
</ul>
<p>问题：如果每输入一个样本就更新参数，网络训练速度过慢；单一数据带来的随机性，使得算法收敛缓慢</p>
<p>改进：采用随机梯度下降法(SGD)。输入一批样本(batch或mini-batch)，求出这些样本的梯度平均值后根据这个平均值改变参数。在神经网络训练中，batch的样本数(batchsize)大致设置为50~200不等。对于所有训练数据，根据batch size分割为各个不同的batch。一个epoch指的是按照batch遍历所有训练样本一次。实际训练中，往往训练多个epoch，对于每一个epoch需要随机打乱所有训练样本的次序，增加batch中训练样本的随机性。</p>
<h2 id="5-强化学习"><a href="#5-强化学习" class="headerlink" title="5 强化学习"></a>5 强化学习</h2><h3 id="5-1-传统强化学习算法"><a href="#5-1-传统强化学习算法" class="headerlink" title="5.1 传统强化学习算法"></a>5.1 传统强化学习算法</h3><p>Q-Learning 算法</p>
<p>epsilion-greedy 算法</p>
<h3 id="5-2-深度强化学习"><a href="#5-2-深度强化学习" class="headerlink" title="5.2 深度强化学习"></a>5.2 深度强化学习</h3><p>蒙特卡洛算法：是利用一个回合结束后得到的奖励来更新当前的Q值；</p>
<p>时序差分更新：希望可以尽早的更新Q值，而不是只有等到一个回合结束之后才能更新。</p>
<p>两者更新公式的常用形式分别如下：</p>
<p><img src="https://panchuang.net/wp-content/uploads/2019/11/1-1573310259.png" alt="img"></p>
<p><img src="https://panchuang.net/wp-content/uploads/2019/11/9-1573310259.png" alt="img"></p>
<p>$Q(s_t,a_t)$ 的值表示 agent 在状态 $s_t$ 下，执行动作 $a_t$ 后，沿着当前策略走下去后所能得到的累积奖励的期望，是对奖励的一个估计值。所以从处理奖励值的角度看：蒙特卡洛算法中走完一个回合后得到的G是真实的奖励值，而时序差分学习则是用估计的奖励值替代真实的奖励值来更新。</p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://sibyl233.github.io/2020/02/25/机器学习笔记/" title="机器学习笔记" target="_blank" rel="external">http://sibyl233.github.io/2020/02/25/机器学习笔记/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/cofess" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/cofess" target="_blank"><span class="text-dark">昵称</span><small class="ml-1x">Web Developer &amp; Designer</small></a></h3>
        <div>个人简介。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    
    <li class="next">
      <a href="/2020/02/20/Photoshop精简操作/" title="Photoshop精简操作"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/cofess" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://weibo.com/cofess" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script src="/js/plugin.min.js"></script>
<script src="/js/application.js"></script>

    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>





   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: '',
    appKey: '',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

     







</body>
</html>