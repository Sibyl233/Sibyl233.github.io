<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习笔记]]></title>
    <url>%2F2020%2F02%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1 引言1.1 机器学习定义第一个定义——Arthur Samuel对机器学习的定义： 机器学习是这样的领域，它赋予计算机学习的能力，这种学习能力不是通过显著式编程获得的。 显著式编程：比如发一系列指令（左转-开门-右转……）让机器人到教室门外的咖啡机前冲咖啡，其劣势是需要把机器人所处的环境调查得一清二楚， 非显著式编程：让计算机自己总结规律的编程方法。比如规定行为（如左转、后退等）和收益函数（如摔倒时收益函数为负值），让计算机自己去找最大化收益函数的行为模式。 第二个定义——Tom Mitshell于1998年在他的书《Machine Learning》中的定义 一个计算机程序被称为可以学习，是指它能够针对某个任务T和某个性能指标P，从经验E中学习。这种学习的特点是，它在T上的被P所衡量的性能，会随着经验E的增加而提高。 任务（Task, T） 经验（Experience, E） 性能指标（Performance Measure, P） 1.2 机器学习任务的分类按照任务是否需要和环境交互获得经验分为监督学习和强化学习两类。比如在以下四个任务中，2和3属于监督学习，所有经验E都是人工采集（训练数据+标签）并输入计算机；1和4属于强化学习，经验E是由计算机与环境互动获得的。不过这样的划分并不绝对，比如在ALPHAGO中，首先通过监督学习（棋局）获得初始围棋程序，再通过强化学习提升棋力。 教计算机下棋 垃圾邮件识别 人脸识别 无人驾驶 对于监督学习可以进一步分类。 按照训练数据是否存在标签分为 传统监督学习 支持向量机 人工神经网络 深度神经网络 无监督学习 聚类 EM算法 主成分分析 半监督学习 按照标签是连续还是离散分为 分类 回归 1.3 机器学习算法的过程特征提取、特征选择 → 不同的算法对特征空间做不同的划分 → 不同的结果 1.4 没有免费午餐定理1995年，D.H.Wolpert等人提出没有免费午餐定理（No Free Lunch Theorem）： 任何一个预测函数，如果在一些训练样本上表现好，那么必然在另一些训练样本上表现不好，如果不对数据在特征空间的先验分布有一定假设，那么表现好与表现不好的情况一样多。 即如果不对特征空间的先验分布有假设，那么所有算法的表现都是一样的。再好的算法也有犯错的风险，因为机器学习的本质是通过有限的已知数据在复杂的高维特征空间中预测未知的样本，然而没有人知道未知样本在哪里，性质到底如何；也没有放之四海而皆准的最好算法，因为评价算法的好坏涉及到对特征空间先验分布的假设，然而没有人知道先验分布的真实样子。 在设计机器学习算法时的常用假设： 在特征空间上距离接近的样本，他们属于同一个类别的概率会更高。 2 支持向量机2.1 问题描述首先，明确线性可分和非线性可分的基础概念。 直观定义： 对于二维特征空间，存在一条直线将训练样本分开为线性可分，反之为非线性可分； 对于高维特征空间，存在一个超平面将训练样本分开为线性可分，反之为非线性可分。 数学定义： 假设我们有$N$个训练样本和它们的标签： \begin{aligned} &\left\{\left(X_{1}, y_{1}\right),\left(X_{2}, y_{2}\right), \ldots,\left(X_{N}, y_{N}\right)\right\}\\ &X_{i}=\left[x_{i 1}, x_{i 2}\right]^{T}\\ &y_{i}=\{+1,-1\} \end{aligned}线性可分是指存在 $(\omega,b)$ 使得对 $i=1\sim N$ 有 y_{i}\left(\omega^{T} X_{i}+b\right)>01995年，Vladimir Vapnik提出支持向量机寻找的最优分类直线应满足以下3个条件。 该直线分开了两类 该直线最大化间隔（margin） 该直线处于间隔的中间，到所有支持向量的距离相等 此时我们把样本点中与分离超平面距离最近的数据点称为支持向量。在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用。如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响。 2.2 线性可分-硬间隔假定训练样本是线性可分的，该优化问题可以写成如下形式： \begin{eqnarray*} \text {Minimize: }&& \frac{1}{2}\|\omega\|^{2} \tag{2.2.1}\\ \text{s.t.: }&& {y}_{i}\left(\omega^{T} x_{i}+b\right) \geq 1,(i=1 \sim N) \tag{2.2.2} \end{eqnarray*}其中待求的是 $\omega$ 和 $b$ ，该形式的推导过程如下。 [事实1] $(\omega,b)$ 表示的超平面和 $(a\omega,ab)$ 表示的超平面是同一个平面。所以我们可以用参数 $a$ 来缩放 $(\omega,b)$，使它满足下式。 \begin{aligned} &\left|\omega^{T} x_{0}+b\right|=1 \quad \text{on support vector}\ x_{0}\\ &\left|\omega^{T} x_{0}+b\right|>1 \quad \text{on non-support vectors} \end{aligned}限制条件 (2.2.2) 中 $y_{i}$ 可以协调超平面的左右，使得对于非支持向量分布于超平面的两侧。 [事实2] 支持向量到超平面的距离公式众所周知。结合事实1可得到下式，所以要最大化支持向量到超平面的距离就等价于最小化 $|\omega|$。 d=\frac{\left|\omega^{T} x_{0}+b\right|}{\|\omega\|}=\frac{1}{\|\omega\|} 最后目标函数的形式如 (2.2.1) 所示，主要是为了求导的方便。 2.3 非线性可分-软间隔在线性不可分情况下，不存在 $(\omega,b)$ 满足如 (2.2.2) 的限制条件，所以需要适当放松限制条件，引入松弛变量 $\delta{i}$ ，同时也要限制 $\delta{i}$ 的范围避免其无限大。改造后的优化问题如下所示： \begin{eqnarray*} \text {Minimize: }&& \frac{1}{2}\|\omega\|^{2}+C \sum_{i=1}^{N} \delta_{i}^{2}\\ \text{s.t.: }&&(1)\ \delta_{i} \geq 0,(i=1 \sim N)\\ &&(2)\ y_{i}\left(\omega^{T} X_{i}+b\right) \geq 1-\delta_{i},(i=1 \sim N) \end{eqnarray*}其中 $(\omega,b,\delta_{i})$ 为待求项，比例因子 $C$ 作为超参数 (Hyper Parameter) 需要人为设定，起到了平衡两项的作用。 2.4 非线性可分-核技巧软间隔的方式作用有限，对于非线性可分数据集有时需要曲面，所以只能扩大可选函数的范围使其超越线性。基本思路是将训练样本的特征空间从低维映射到高维，再用线性超平面对数据进行分类。以异或问题为例： 我们不妨先假设这个映射函数为 $\phi(x)$ ，那么优化问题将变为如下形式： \begin{eqnarray*} \text {Minimize: }&& \frac{1}{2}\|\omega\|^{2}+C \sum_{i=1}^{N} \delta_{i}^{2}\\ \text{s.t.: }&&(1)\ \delta_{i} \geq 0,(i=1 \sim N)\\ &&(2)\ y_{i}\left[\omega^{T} \varphi(X_{i})+b\right] \geq 1-\delta_{i},(i=1 \sim N) \end{eqnarray*}求解上述优化问题的对偶问题涉及到计算 $\varphi\left(X{1}\right)^{T} \varphi\left(X{2}\right)$。由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算 $\varphi\left(X{1}\right)^{T}\varphi\left(X{2}\right)$ 通常是困难的，为了避开这个障碍，我们引入核函数 $K$。后续章节将说明我们不需要显式地定义映射 $\varphi $ 是什么，而只需事先定义核函数 $K$ ，就可以利用解线性问题的方法求解非线性问题的支持向量机。本节首先对核函数 $K$ 有个初步的了解。 核函数的定义：$K\left(X{1}, X{2}\right)=\varphi\left(X{1}\right)^{T} \varphi\left(X{2}\right)$ 核函数的充要条件：①交换性；②半正定性 。（Mercer定理） 核函数 $K$ 和映射函数 $\phi $ 一一对应。 常用核函数： 2.5 原问题和对偶问题 原问题（Prime Problem） 其中自变量 $\omega$ 为多维向量，$f(\omega)$为目标函数，$g{i}(\omega)$ 和 $h{i}(\omega)$为限制条件。 \begin{aligned} \text {Minimize: }& f(\omega) \\ \text{s.t.: }& g_{i}(\omega) \leq 0 \quad i=1 \sim K \\ &h_{i}(\omega)=0 \quad i=1 \sim m \end{aligned} 对偶问题（Dual Problem） 首先定义函数 \begin{aligned} L(\omega, \alpha, \beta) &=f(\omega)+\sum_{i=1}^{K} \alpha_{i} g_{i}(\omega)+\sum_{i=1}^{M} \beta_{i} h_{i}(\omega) \\ &=f(\omega)+\alpha^{T} g(\omega)+\beta^{T} h(\omega) \end{aligned}在此基础上定义对偶问题如下,其中 $\theta(\alpha, \beta)$ 表示在遍历所有的 $\omega$ 后取 $L(\omega, \alpha, \beta)$ 的最小值 \begin{aligned} \text{Maxmize: }& \theta(\alpha, \beta)=\inf_{\omega} \ L(\omega, \alpha, \beta) \\ \text{s.t.: }& \alpha_{i} \geq 0, i=1 \sim K \end{aligned} 原问题与对偶问题的关系 [定理1] 弱对偶性：如果 $\omega^$ 是原问题的解，$\alpha^$ 和 $\beta^*$ 是对偶问题的解，那么 f\left(\omega^{*}\right) \geqslant \theta\left(\alpha^{*}, \beta^{*}\right)证明： \begin{aligned} \theta\left(\alpha^{*}, \beta^{*}\right) &=\inf L\left(\omega, \alpha^{*}, \beta^{*}\right) \\ & \leq L\left(w^{*}, \alpha^{*}, \beta^{*}\right) \\ &=f\left(\omega^{*}\right)+\alpha^{* T} g\left(\omega^{*}\right)+\beta^{* T} h\left(\omega^{*}\right) \\ & \leq f\left(\omega^{*}\right) \end{aligned}[定理2] 强对偶性：如果 $g(\omega)=A\omega+b$，$h(\omega)=C\omega+d$，$f(\omega)$ 为凸函数，那么 f\left(\omega^{*}\right)=\theta\left(\alpha^{*}, \beta^{*}\right)证明略，见附录1。 [定义1] Slater条件：即强对偶性成立的条件。 [定义2] KKT条件：对所有 $i=1\sim K$ 要么 $\alpha_{i}^{}=0$， 要么 $g(\omega^{})=0$ 推导：由定理1证明部分的第3行到第4行易知，要取等号必须满足KKT条件。 注释：KKT是三人名字的首字母。 2.6 SVM转化为对偶问题2.7 算法总体流程接着2.6节，总结支持向量机训练和测试的流程。 训练过程 （1）输入训练数据 \left\{\left(X_{i}, y_{i}\right)\right\},\text{where }y_{i}=\{+1,-1\}（2）用SMO算法求解如下优化问题，解出 $\alpha_{i}$ \begin{eqnarray*} \text{Maxmize: }&& \theta(\alpha, \beta)=\sum_{i=1}^{N} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} y_{i} y_{j} \alpha_{i} \alpha_{j} \varphi\left(X_{i}\right)^{\mathrm{T}} \varphi\left(X_{j}\right) \tag{2.7.1}\\ \text{s.t.: }&& (1)\ 0\leq \alpha_{i} \leq C, \ i=1 \sim N \\ &&(2)\ \sum_{i=1}^{N} \alpha_{i} y_{i}=0, \ i=1 \sim N \end{eqnarray*}（3）找一个 $\alpha{i}\neq 0$ 且 $\alpha{i}\neq C$ ，利用下式求 $b$ b=\frac{1-\sum_{j=1}^{N} \alpha_{j} y_{i} y_{j} K\left(X_{j}, X_{i}\right)}{y_{i}} \tag{2.7.2} 测试过程 考察测试数据 $X$，预测其类别 $y$ \begin{aligned} &\text{if }\sum_{i=1}^{N} \alpha_{i} y_{i} K\left(X_{i}, X\right)+b \geq 0, \quad \text{then } y=+1\\ &\text{if }\sum_{i=1}^{N} \alpha_{i} y_{i} K\left(X_{i}, X\right)+b]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Photoshop精简操作]]></title>
    <url>%2F2020%2F02%2F20%2FPhotoshop%E7%B2%BE%E7%AE%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1 PS设计基础 1.1 工作区和工作流程工作区：上方为菜单栏，左侧为工具栏，右侧为功能面板工作流程：同一个任务往往有多种操作方式 例：图像亮度调整 1. 图像菜单 &gt;&gt; 调整 &gt;&gt; 亮度/对比度2. 图像菜单 &gt;&gt; 调整 &gt;&gt; 亮度/色阶3. 获取亮度选区 + 混合模式 1.2 色彩基础和吸管工具组色彩基础 RGB色轮：光的三原色 伊顿色轮：美术三原色 吸管工具组 吸管工具：取样点的选择。默认单个像素，也可以取3x3平均，31x31平均等 颜色取样器工具：配合信息面板获得跟踪颜色取样值 信息面板 删除取样点 窗口菜单 &gt;&gt; 信息面板 1. 按住Option点击取样点2. 点击选项面板”清楚全部“按钮 标尺工具：配合信息面板获得两点间的标尺线段信息 计数工具：在适当位置进行标记，有计数编组等功能 1.3 拾色器和色彩空间拾色器：可通过点击工具栏的前景色进入。常见的四种色彩模型如下 HSB：色相/饱和度/亮度 Lab：明度/a分量/b分量，a从品红到绿色，b从蓝色到黄色 RGB：红/绿/蓝 CMYK：青/洋红/黄/黑（定调色Key），即三原色的补色+黑色 HSB取色步骤 1. 选择色相值，对应色轮上的度数2. 选择饱和度，可以理解为颜色中有多少白色3. 选择亮度，可以理解为颜色中有多少黑色 通过颜色库取色，通常用于印刷场合。 冷知识：世界上最丑的颜色 Pantone 448 C（Pantone是最大的色彩厂商），一种深棕色 色彩空间 = 色彩模型 + 色域。如果把色彩模型理解为用数字来描述颜色，色彩空间则需要和一个具体的设备关联起来（如肉眼、电脑屏幕等） 1.4 数字图像处理：尺寸更改图像标签页显示的信息有图像的缩放率（图像窗口左下角可手动输入），色彩模式和通道数（对应图像菜单 &gt;&gt; 模式）。图像窗口左下角还可以指定的常用信息有 文档大小：图像文档在PS内部以无损方式存储所占用的体积 文档尺寸：图像文档的宽和高在某个度量单位下的数值 文档配置文件：对应于编辑菜单 &gt;&gt; 指定配置文件/转换为配置文件 有必要了解虚拟单位、实际单位和分辨率的概念 虚拟单位：像素、点、百分比 实际单位：英寸、厘米、毫米 分辨率：PPI（每英寸像素数）、DPI（每英寸点数） 显示打印尺寸 为了该命令能发挥正常功能，需要先更改预设分辨率。1. 首选项面板 &gt;&gt; 单位与标尺：填写正确的与当前设备屏幕属性一致的屏幕分辨率数值。2. 视图菜单 &gt;&gt; 打印尺寸：根据预设参数，在屏幕上显示图像文档的实际尺寸，即物理尺寸。 其中屏幕分辨率的计算：比如屏幕宽度的虚拟尺寸为1440像素，实际尺寸为15英寸，那么屏幕分辨率可估算为1440/15=96像素/英寸（之所以说是估算因为15英寸指的是对角线长） 尺寸调整相关 1. 图像菜单 &gt;&gt; 图像大小：设置图像的宽、高和分辨率。注意等比例缩放和重新采样选项。2. 图像菜单 &gt;&gt; 画布大小：设置画布的宽和高。推荐勾选相对选项。3. 图像菜单/工具栏 &gt;&gt; 裁剪：同时裁剪图像和画布大小。注意不要勾选删除裁剪的像素选项。 1.5 数字图像处理：文件格式 体积 视觉损失 透明 特性 JPG/JPEG 较小 有损 不支持 常用于照片、绘画 PNG 较大 无损 索引透明或Alpha透明 常用于资源类图像 GIF 中等较大 索引颜色 索引透明 常用于动画、图标 JPG格式不支持透明度，透明区域将被自动填充为背景色. PNG-8格式支持索引透明，每个像素只能是全透明（100%透明）或者不透明。 PNG-24格式支持Alpha透明，每个像素都拥有256级别的透明度。 GIF格式支持索引透明；拥有至多256种颜色（索引颜色）；支持动画。 Tip 使用文件菜单 &gt;&gt; 导出 &gt;&gt; 存储为Web所有格式（旧版）…便于观察最后结果和相互对比 2 绘画、修饰、选择工具和选取初步2.1 画笔工具组2.2 历史记录画笔2.3 橡皮擦和油漆桶工具组2.4 仿制图章和修饰工具组2.5 选区初步]]></content>
      <categories>
        <category>指南</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Zotero简明使用指南]]></title>
    <url>%2F2020%2F01%2F08%2FZotero%E7%AE%80%E6%98%8E%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[一款免费易用的文献管理工具 下载与基本设置 下载官网下载Zotero和Zotero Connector。前者是管理文献的本体；后者是用于抓取条目和相关附件的浏览器插件。 设置同步Zotero本身自带同步功能，但是自带空间有限，故可以使用webdav连接到其他网盘，以坚果云为例：如何在Zotero中设置webdav连接到坚果云？ 设置ZotfileZotfile是一款插件，配合Zotero使用可以更好地管理（批量命名和移除等）文献附件：下载和设置。 其它功能 导出参考文献（Word/Latex）若不使用插件，❶直接拖拽条目至文本编辑器/使用快捷键即可实现单条目的快速复制；❷在Zotero中选取所需条目，右键选择由所选条目创建引文目录即可实现多条目的快速复制。若使用插件，以Word为例，在成功安装插件的前提下Word菜单栏中会出现Zotero选项卡，可方便地添加引用和参考文献。 Papership（ipad/iphone端）Zotero只有PC端应用，配合Papership使用可以弥补该不足。用Zotero账户登录Papership应用，并设置Zotero File Hosting（与步骤1.2设置同步类似）就可以在平板和手机上同步阅读文献了。]]></content>
      <categories>
        <category>指南</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语义分割方法整理]]></title>
    <url>%2F2019%2F09%2F23%2FSemantic-Segmentation%2F</url>
    <content type="text"><![CDATA[持续更新 Basis FCN [FCN] Fully Convolutional Networks for Semantic Segmentation DeepLab [DeepLab v1] Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs [DeepLab v2] DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs [DeepLab v3] Rethinking Atrous Convolution for Semantic Image Segmentation [DeepLab v3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Encoder-decoder Architecture [U-Net] U-Net: Convolutional Networks for Biomedical Image Segmentation [SegNet] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation [DeconvNet] Learning Deconvolution Network for Semantic Segmentation [RefineNet] RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation MethodCRF [CRFasRNN] Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials [MRF] Semantic Image Segmentation via Deep Parsing Network [GRF] Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs Atrous/Dilated Convolution [DUC-HDC] Understanding Convolution for Semantic Segmentation [DRN] Dilated Residual Networks Smoothed Dilated Convolutions for Improved Dense Prediction Efficient Smoothing of Dilated Convolutions for Image Segmentation [FastFCN] FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Context Aggregation Pooling [ParseNet] ParseNet: Looking Wider to See Better [PSPNet] Pyramid Scene Parsing Network [DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes [VortexPooling] Vortex Pooling: Improving Context Representation in Semantic Segmentation Large Kernel [GCN] Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network [ExFuse] ExFuse: Enhancing Feature Fusion for Semantic Segmentation Attention Mechanism Channel Reweighting [DFN] Learning a Discriminative Feature Network for Semantic Segmentation [EncNet] Context Encoding for Semantic Segmentation[ [SENet] Squeeze-and-Excitation Networks Pyramid Attention Network for Semantic Segmentation Spatial Attention [OCNet] OCNet: Object Context Network for Scene Parsing [DANet] Dual Attention Networks for Multimodal Reasoning and Matching [PSANet] PSANet: Point-wise Spatial Attention Network for Scene Parsing [CCNet] CCNet: Criss-Cross Attention for Semantic Segmentation Graph Convolution [GloRe] Graph-Based Global Reasoning Networks Beyond Grids: Learning Graph Representations for Visual Recognition Real-time MethodConvolution Factorization [ENet] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation [ERFNet] ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation [ESPNet] ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation [ESNet] ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation [LEDNet] LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation [DABNet] DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation Multi-branch [ICNet] ICNet for Real-Time Semantic Segmentation on High-Resolution Images [ContextNet] ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time [BiSeNet] BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation Reference 语义分割论文整理 [blog] Survey on semantic segmentation using deep learning techniques [paper]]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[道路场景语义分割]]></title>
    <url>%2F2019%2F07%2F11%2F%E9%81%93%E8%B7%AF%E5%9C%BA%E6%99%AF%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[持续更新 Datasets Apolloscape Scene Parsing BDD100k CamVid Cityscapes Daimler Urban Segmentation Kitti Mapillary Vistas Method Method Cityscapes(%) CamVid(%) Publication FCN-8s 65.3 CVPR2015 SegNet 50.2 arXiv15 DeepLab v2 70.4 PAMI2018 G-FRNet 68.0 CVPR2017 FRRN B 71.8 CVPR2017 RefineNet 73.6 CVPR2017 GCN 76.9 CVPR2017 TKCN 79.5 ICME2019 DUC-HDC 80.1 WACV2018 PSANet 80.1 ECCV2018 PSPNet 80.2 CVPR2017 DDSC 70.9 CVPR2018 DFN 80.3 CVPR2018 DenseASPP 80.6 CVPR2018 LDN 80.6 78.1 arXiv19 GloRe 80.9 CVPR2019 OCNet 81.2 arXiv18 DeepLab v3 81.3 arXiv17 CCNet 81.4 arXiv18 DAN 81.5 CVPR2019 HRNetV2 81.6 arXiv19 CaseNet 81.9 arXiv19 DeepLab v3+ 82.1 ECCV2018 GFF 82.3 arXiv19 DPC 82.7 NIPS2018 GSCNN 82.8 ICCV2019 Real-time Method Method WxH mIOU(%) FPS GFLOPs Param(M) ENet 1920x1080 58.3 21.6 3.83 0.37 SQ 2048x1024 59.8 - - - TwoColumn 1024x512 72.9 14.7 - - CGNet 2048x1024 64.8 17.6 6 0.5 ESPNet - 60.3 4 - 0.4 ERFNet - 68 - - - ICNet 2048x1024 69.5 30.3 - - BiSeNet 1536x768 68.4 105.8 2.9 5.8 1536x768 74.7 65.5 10.8 49 ESNet - 70.7 63 - 1.66 DFANet 1024x1024 71.3 100 3.4 7.8 LEDNet 1024x512 70.1 71 - 0.94 DABNet 1024x512 70.1 104.2 - 0.76 SqueezeNAS 1024x512 72.5 - - 1.9 *上表为文献中各方法在Cityscapes上的性能]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一个测试]]></title>
    <url>%2F2018%2F05%2F11%2F%E4%B8%80%E4%B8%AA%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[ヾ(๑╹◡╹)ﾉ”]]></content>
      <categories>
        <category>胡言</category>
      </categories>
  </entry>
</search>
