<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>机器学习笔记</title>
    <link href="/2020/02/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2020/02/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><a id="more"></a><h3 id="1-1-机器学习定义"><a href="#1-1-机器学习定义" class="headerlink" title="1.1 机器学习定义"></a>1.1 机器学习定义</h3><p>第一个定义——Arthur Samuel对机器学习的定义：</p><blockquote><p>机器学习是这样的领域，它赋予计算机学习的能力，这种学习学习能力不是通过显著式编程获得的。</p></blockquote><ul><li>显著式编程：比如发一系列指令（左转-开门-右转……）让机器人到教室门外的咖啡机前冲咖啡，其劣势是需要把机器人所处的环境调查得一清二楚，</li><li>非显著式编程：让计算机自己总结规律的编程方法。比如规定行为（如左转、后退等）和收益函数（如摔倒时收益函数为负值），让计算机自己去找最大化收益函数的行为模式。</li></ul><p>第二个定义——Tom Mitshell于1998年在他的书《Machine Learning》中的定义</p><blockquote><p>一个计算机程序被称为可以学习，是指它能够针对某个任务T和某个性能指标P，从经验E中学习。这种学习的特点是，它在T上的被P所衡量的性能，会随着经验E的增加而提高。</p></blockquote><ul><li>任务（Task, T）</li><li>经验（Experience, E）</li><li>性能指标（Performance Measure, P）</li></ul><h3 id="1-2-机器学习任务的分类"><a href="#1-2-机器学习任务的分类" class="headerlink" title="1.2 机器学习任务的分类"></a>1.2 机器学习任务的分类</h3><p>按照任务是否需要和环境交互获得经验分为<strong>监督学习</strong>和<strong>强化学习</strong>两类。比如在以下四个任务中，2和3属于监督学习，所有经验E都是人工采集（训练数据+标签）并输入计算机；1和4属于强化学习，经验E是由计算机与环境互动获得的。不过这样的划分并不绝对，比如在ALPHAGO中，首先通过监督学习（棋局）获得初始围棋程序，再通过强化学习提升棋力。</p><ol><li>教计算机下棋</li><li>垃圾邮件识别</li><li>人脸识别</li><li>无人驾驶</li></ol><p>对于<strong>监督学习</strong>可以进一步分类。</p><ul><li>按照训练数据是否存在标签分为<ul><li>传统监督学习<ul><li>支持向量机</li><li>人工神经网络</li><li>深度神经网络</li></ul></li><li>无监督学习<ul><li>聚类</li><li>EM算法</li><li>主成分分析</li></ul></li><li>半监督学习</li></ul></li><li>按照标签是连续还是离散分为<ul><li>分类</li><li>回归</li></ul></li></ul><h3 id="1-3-机器学习算法的过程"><a href="#1-3-机器学习算法的过程" class="headerlink" title="1.3 机器学习算法的过程"></a>1.3 机器学习算法的过程</h3><p>特征提取、特征选择 → 不同的算法对特征空间做不同的划分 → 不同的结果</p><h3 id="1-4-没有免费午餐定理"><a href="#1-4-没有免费午餐定理" class="headerlink" title="1.4 没有免费午餐定理"></a>1.4 没有免费午餐定理</h3><p>1995年，D.H.Wolpert等人提出没有免费午餐定理（No Free Lunch Theorem）：</p><blockquote><p>任何一个预测函数，如果在一些训练样本上表现好，那么必然在另一些训练样本上表现不好，如果不对数据在特征空间的先验分布有一定假设，那么表现好与表现不好的情况一样多。</p></blockquote><p>即如果不对特征空间的先验分布有假设，那么所有算法的表现都是一样的。再好的算法也有犯错的风险，因为机器学习的本质是通过有限的已知数据在复杂的高维特征空间中预测未知的样本，然而没有人知道未知样本在哪里，性质到底如何；也没有放之四海而皆准的最好算法，因为评价算法的好坏涉及到对特征空间先验分布的假设，然而没有人知道先验分布的真实样子。</p><p>在设计机器学习算法时的常用假设：</p><blockquote><p>在特征空间上距离接近的样本，他们属于同一个类别的概率会更高。</p></blockquote><h2 id="2-支持向量机"><a href="#2-支持向量机" class="headerlink" title="2 支持向量机"></a>2 支持向量机</h2><h3 id="2-1-问题描述"><a href="#2-1-问题描述" class="headerlink" title="2.1 问题描述"></a>2.1 问题描述</h3><p>首先，明确线性可分和非线性可分的基础概念。</p><ul><li><p>直观定义：</p><ul><li>对于二维特征空间，存在一条直线将训练样本分开为线性可分，反之为非线性可分；</li><li>对于高维特征空间，存在一个超平面将训练样本分开为线性可分，反之为非线性可分。</li></ul></li><li><p>数学定义：</p><ul><li><p>假设我们有N个训练样本和它们的标签：</p><script type="math/tex; mode=display">\begin{aligned}&\left\{\left(X_{1}, y_{1}\right),\left(X_{2}, y_{2}\right), \ldots,\left(X_{N}, y_{N}\right)\right\}\\&X_{i}=\left[x_{i 1}, x_{i 2}\right]^{T}\\&y_{i}=\{+1,-1\}\end{aligned}</script></li><li><p>线性可分是指存在$(w,b)$使得对$i=1～N$ 有</p><script type="math/tex; mode=display">y_{i}\left(\omega^{T} X_{i}+b\right)>0</script></li></ul></li></ul><h3 id="2-2-线性可分"><a href="#2-2-线性可分" class="headerlink" title="2.2 线性可分"></a>2.2 线性可分</h3><h3 id="2-3-非线性可分-软间隔"><a href="#2-3-非线性可分-软间隔" class="headerlink" title="2.3 非线性可分-软间隔"></a>2.3 非线性可分-软间隔</h3><h3 id="2-4-非线性可分-核函数"><a href="#2-4-非线性可分-核函数" class="headerlink" title="2.4 非线性可分-核函数"></a>2.4 非线性可分-核函数</h3><h3 id="2-5-原问题和对偶问题"><a href="#2-5-原问题和对偶问题" class="headerlink" title="2.5 原问题和对偶问题"></a>2.5 原问题和对偶问题</h3><h3 id="2-6-SVM转化为对偶问题"><a href="#2-6-SVM转化为对偶问题" class="headerlink" title="2.6 SVM转化为对偶问题"></a>2.6 SVM转化为对偶问题</h3><h3 id="2-7-算法总体流程"><a href="#2-7-算法总体流程" class="headerlink" title="2.7 算法总体流程"></a>2.7 算法总体流程</h3>]]></content>
    
    
    <categories>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Photoshop精简操作</title>
    <link href="/2020/02/20/Photoshop%E7%B2%BE%E7%AE%80%E6%93%8D%E4%BD%9C/"/>
    <url>/2020/02/20/Photoshop%E7%B2%BE%E7%AE%80%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="1-PS设计基础"><a href="#1-PS设计基础" class="headerlink" title="1 PS设计基础 "></a>1 PS设计基础 <a id="more"></a></h2><h3 id="1-1-工作区和工作流程"><a href="#1-1-工作区和工作流程" class="headerlink" title="1.1 工作区和工作流程"></a>1.1 工作区和工作流程</h3><p>工作区：上方为菜单栏，左侧为工具栏，右侧为功能面板<br>工作流程：同一个任务往往有多种操作方式</p><div class="table-container"><table><thead><tr><th>例：图像亮度调整</th></tr></thead><tbody><tr><td>1. 图像菜单 &gt;&gt; 调整 &gt;&gt; 亮度/对比度<br>2. 图像菜单 &gt;&gt; 调整 &gt;&gt; 亮度/色阶<br>3. 获取亮度选区 + 混合模式</td></tr></tbody></table></div><h3 id="1-2-色彩基础和吸管工具组"><a href="#1-2-色彩基础和吸管工具组" class="headerlink" title="1.2 色彩基础和吸管工具组"></a>1.2 色彩基础和吸管工具组</h3><p>色彩基础</p><ul><li>RGB色轮：光的三原色</li><li>伊顿色轮：美术三原色</li></ul><p>吸管工具组</p><ul><li>吸管工具：取样点的选择。默认单个像素，也可以取3x3平均，31x31平均等</li><li>颜色取样器工具：配合信息面板获得跟踪颜色取样值</li></ul><div class="table-container"><table><thead><tr><th>信息面板</th><th>删除取样点</th></tr></thead><tbody><tr><td>窗口菜单 &gt;&gt; 信息面板</td><td>1. 按住Option点击取样点<br>2. 点击选项面板”清楚全部“按钮</td></tr></tbody></table></div><ul><li>标尺工具：配合信息面板获得两点间的标尺线段信息</li><li>计数工具：在适当位置进行标记，有计数编组等功能</li></ul><h3 id="1-3-拾色器和色彩空间"><a href="#1-3-拾色器和色彩空间" class="headerlink" title="1.3 拾色器和色彩空间"></a>1.3 拾色器和色彩空间</h3><p>拾色器：可通过点击工具栏的前景色进入。常见的四种色彩模型如下</p><ul><li>HSB：色相/饱和度/亮度</li><li>Lab：明度/a分量/b分量，a从品红到绿色，b从蓝色到黄色</li><li>RGB：红/绿/蓝</li><li>CMYK：青/洋红/黄/黑（定调色Key），即三原色的补色+黑色</li></ul><div class="table-container"><table><thead><tr><th>HSB取色步骤</th></tr></thead><tbody><tr><td>1. 选择色相值，对应色轮上的度数<br>2. 选择饱和度，可以理解为颜色中有多少白色<br>3. 选择亮度，可以理解为颜色中有多少黑色</td></tr></tbody></table></div><p>通过颜色库取色，通常用于印刷场合。</p><div class="table-container"><table><thead><tr><th>冷知识：世界上最丑的颜色</th></tr></thead><tbody><tr><td>Pantone 448 C（Pantone是最大的色彩厂商），一种深棕色</td></tr></tbody></table></div><p>色彩空间 = 色彩模型 + 色域。如果把色彩模型理解为用数字来描述颜色，色彩空间则需要和一个具体的设备关联起来（如肉眼、电脑屏幕等）</p><h3 id="1-4-数字图像处理：尺寸更改"><a href="#1-4-数字图像处理：尺寸更改" class="headerlink" title="1.4 数字图像处理：尺寸更改"></a>1.4 数字图像处理：尺寸更改</h3><p>图像标签页显示的信息有图像的缩放率（图像窗口左下角可手动输入），色彩模式和通道数（对应图像菜单 &gt;&gt; 模式）。图像窗口左下角还可以指定的常用信息有</p><ul><li>文档大小：图像文档在PS内部以无损方式存储所占用的体积</li><li>文档尺寸：图像文档的宽和高在某个度量单位下的数值</li><li>文档配置文件：对应于编辑菜单 &gt;&gt; 指定配置文件/转换为配置文件</li></ul><p>有必要了解虚拟单位、实际单位和分辨率的概念</p><ul><li>虚拟单位：像素、点、百分比</li><li>实际单位：英寸、厘米、毫米</li><li>分辨率：PPI（每英寸像素数）、DPI（每英寸点数）</li></ul><div class="table-container"><table><thead><tr><th>显示打印尺寸</th></tr></thead><tbody><tr><td>为了该命令能发挥正常功能，需要先更改预设分辨率。<br>1. 首选项面板 &gt;&gt; 单位与标尺：填写正确的与当前设备屏幕属性一致的屏幕分辨率数值。<br>2. 视图菜单 &gt;&gt; 打印尺寸：根据预设参数，在屏幕上显示图像文档的实际尺寸，即物理尺寸。</td></tr></tbody></table></div><p>其中屏幕分辨率的计算：比如屏幕宽度的虚拟尺寸为1440像素，实际尺寸为15英寸，那么屏幕分辨率可估算为1440/15=96像素/英寸（之所以说是估算因为15英寸指的是对角线长）</p><div class="table-container"><table><thead><tr><th>尺寸调整相关</th></tr></thead><tbody><tr><td>1. 图像菜单 &gt;&gt; 图像大小：设置图像的宽、高和分辨率。注意<u>等比例缩放</u>和<u>重新采样</u>选项。<br>2. 图像菜单 &gt;&gt; 画布大小：设置画布的宽和高。推荐勾选<u>相对</u>选项。<br>3. 图像菜单/工具栏 &gt;&gt; 裁剪：同时裁剪图像和画布大小。注意不要勾选<u>删除裁剪的像素</u>选项。</td></tr></tbody></table></div><h3 id="1-5-数字图像处理：文件格式"><a href="#1-5-数字图像处理：文件格式" class="headerlink" title="1.5 数字图像处理：文件格式"></a>1.5 数字图像处理：文件格式</h3><div class="table-container"><table><thead><tr><th></th><th>体积</th><th>视觉损失</th><th>透明</th><th>特性</th></tr></thead><tbody><tr><td>JPG/JPEG</td><td>较小</td><td>有损</td><td>不支持</td><td>常用于照片、绘画</td></tr><tr><td>PNG</td><td>较大</td><td>无损</td><td>索引透明或Alpha透明</td><td>常用于资源类图像</td></tr><tr><td>GIF</td><td>中等较大</td><td>索引颜色</td><td>索引透明</td><td>常用于动画、图标</td></tr></tbody></table></div><ul><li>JPG格式不支持透明度，透明区域将被自动填充为背景色.</li><li>PNG-8格式支持<u>索引透明</u>，每个像素只能是全透明（100%透明）或者不透明。</li><li>PNG-24格式支持<u>Alpha透明</u>，每个像素都拥有256级别的透明度。</li><li>GIF格式支持索引透明；拥有至多256种颜色（索引颜色）；支持动画。</li></ul><div class="table-container"><table><thead><tr><th>Tip</th></tr></thead><tbody><tr><td>使用文件菜单 &gt;&gt; 导出 &gt;&gt; 存储为Web所有格式（旧版）…便于观察最后结果和相互对比</td></tr></tbody></table></div><h2 id="2-绘画、修饰、选择工具和选取初步"><a href="#2-绘画、修饰、选择工具和选取初步" class="headerlink" title="2 绘画、修饰、选择工具和选取初步"></a>2 绘画、修饰、选择工具和选取初步</h2><h3 id="2-1-画笔工具组"><a href="#2-1-画笔工具组" class="headerlink" title="2.1 画笔工具组"></a>2.1 画笔工具组</h3><h3 id="2-2-历史记录画笔"><a href="#2-2-历史记录画笔" class="headerlink" title="2.2 历史记录画笔"></a>2.2 历史记录画笔</h3><h3 id="2-3-橡皮擦和油漆桶工具组"><a href="#2-3-橡皮擦和油漆桶工具组" class="headerlink" title="2.3 橡皮擦和油漆桶工具组"></a>2.3 橡皮擦和油漆桶工具组</h3><h3 id="2-4-仿制图章和修饰工具组"><a href="#2-4-仿制图章和修饰工具组" class="headerlink" title="2.4 仿制图章和修饰工具组"></a>2.4 仿制图章和修饰工具组</h3><h3 id="2-5-选区初步"><a href="#2-5-选区初步" class="headerlink" title="2.5 选区初步"></a>2.5 选区初步</h3>]]></content>
    
    
    <categories>
      
      <category>指南</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Zotero简明使用指南</title>
    <link href="/2020/01/08/Zotero%E7%AE%80%E6%98%8E%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/01/08/Zotero%E7%AE%80%E6%98%8E%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>一款免费易用的文献管理工具<a id="more"></a></p><h4 id="下载与基本设置"><a href="#下载与基本设置" class="headerlink" title="下载与基本设置"></a>下载与基本设置</h4><ul><li><p>下载<br><a href="https://www.zotero.org/download/" target="_blank" rel="noopener">官网下载</a>Zotero和Zotero Connector。前者是管理文献的本体；后者是用于抓取条目和相关附件的浏览器插件。</p></li><li><p>设置同步<br>Zotero本身自带同步功能，但是自带空间有限，故可以使用webdav连接到其他网盘，以坚果云为例：<a href="http://help.jianguoyun.com/?p=3168" target="_blank" rel="noopener">如何在Zotero中设置webdav连接到坚果云？</a></p></li><li><p>设置Zotfile<br>Zotfile是一款插件，配合Zotero使用可以更好地管理（批量命名和移除等）文献附件：<a href="http://zotfile.com/#how-to-install--set-up-zotfile" target="_blank" rel="noopener">下载和设置</a>。</p></li></ul><h4 id="其它功能"><a href="#其它功能" class="headerlink" title="其它功能"></a>其它功能</h4><ul><li>导出参考文献（Word/Latex）<br>若<a href="https://www.zotero.org/support/creating_bibliographies" target="_blank" rel="noopener">不使用插件</a>，❶直接拖拽条目至文本编辑器/使用快捷键即可实现单条目的快速复制；❷在Zotero中选取所需条目，右键选择<code>由所选条目创建引文目录</code>即可实现多条目的快速复制。<br>若<a href="https://www.zotero.org/support/word_processor_integration" target="_blank" rel="noopener">使用插件</a>，以Word为例，在成功安装插件的前提下Word菜单栏中会出现Zotero选项卡，可方便地添加引用和参考文献。</li><li>Papership（ipad/iphone端）<br>Zotero只有PC端应用，配合Papership使用可以弥补该不足。用Zotero账户登录Papership应用，并<a href="http://shazino.freshdesk.com/support/solutions/articles/112464-how-to-sync-papership-and-zotero-" target="_blank" rel="noopener">设置</a>Zotero File Hosting（与步骤1.2设置同步类似）就可以在平板和手机上同步阅读文献了。</li></ul>]]></content>
    
    
    <categories>
      
      <category>指南</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>语义分割方法整理</title>
    <link href="/2019/09/23/Semantic-Segmentation/"/>
    <url>/2019/09/23/Semantic-Segmentation/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>持续更新<a id="more"></a></p><h4 id="Basis"><a href="#Basis" class="headerlink" title="Basis"></a>Basis</h4><ul><li>FCN<ul><li>[FCN] Fully Convolutional Networks for Semantic Segmentation</li></ul></li><li>DeepLab<ul><li>[DeepLab v1] Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs </li><li>[DeepLab v2] DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</li><li>[DeepLab v3] Rethinking Atrous Convolution for Semantic Image Segmentation </li><li>[DeepLab v3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</li></ul></li><li>Encoder-decoder Architecture<ul><li>[U-Net] U-Net: Convolutional Networks for Biomedical Image Segmentation </li><li>[SegNet] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation</li><li>[DeconvNet] Learning Deconvolution Network for Semantic Segmentation</li><li>[RefineNet] RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</li></ul></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h5><ul><li>[CRFasRNN] Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</li><li>[MRF] Semantic Image Segmentation via Deep Parsing Network</li><li>[GRF] Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs</li></ul><h5 id="Atrous-Dilated-Convolution"><a href="#Atrous-Dilated-Convolution" class="headerlink" title="Atrous/Dilated Convolution"></a>Atrous/Dilated Convolution</h5><ul><li>[DUC-HDC] Understanding Convolution for Semantic Segmentation</li><li>[DRN] Dilated Residual Networks</li><li>Smoothed Dilated Convolutions for Improved Dense Prediction</li><li>Efficient Smoothing of Dilated Convolutions for Image Segmentation</li><li>[FastFCN] FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation</li></ul><h5 id="Context-Aggregation"><a href="#Context-Aggregation" class="headerlink" title="Context Aggregation"></a>Context Aggregation</h5><ul><li>Pooling<ul><li>[ParseNet] ParseNet: Looking Wider to See Better</li><li>[PSPNet] Pyramid Scene Parsing Network</li><li>[DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes</li><li>[VortexPooling] Vortex Pooling: Improving Context Representation in Semantic Segmentation</li></ul></li><li>Large Kernel<ul><li>[GCN] Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network</li><li>[ExFuse] ExFuse: Enhancing Feature Fusion for Semantic Segmentation</li></ul></li></ul><h5 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h5><ul><li>Channel Reweighting<ul><li>[DFN] Learning a Discriminative Feature Network for Semantic Segmentation</li><li>[EncNet] Context Encoding for Semantic Segmentation[</li><li>[SENet] Squeeze-and-Excitation Networks</li><li>Pyramid Attention Network for Semantic Segmentation</li></ul></li><li>Spatial Attention<ul><li>[OCNet] OCNet: Object Context Network for Scene Parsing</li><li>[DANet] Dual Attention Networks for Multimodal Reasoning and Matching</li><li>[PSANet] PSANet: Point-wise Spatial Attention Network for Scene Parsing</li><li>[CCNet] CCNet: Criss-Cross Attention for Semantic Segmentation</li></ul></li></ul><h5 id="Graph-Convolution"><a href="#Graph-Convolution" class="headerlink" title="Graph Convolution"></a>Graph Convolution</h5><ul><li>[GloRe] Graph-Based Global Reasoning Networks</li><li>Beyond Grids: Learning Graph Representations for Visual Recognition</li></ul><h4 id="Real-time-Method"><a href="#Real-time-Method" class="headerlink" title="Real-time Method"></a>Real-time Method</h4><h5 id="Convolution-Factorization"><a href="#Convolution-Factorization" class="headerlink" title="Convolution Factorization"></a>Convolution Factorization</h5><ul><li>[ENet] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</li><li>[ERFNet]  ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</li><li>[ESPNet] ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</li><li>[ESNet] ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation</li><li>[LEDNet] LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation</li><li>[DABNet] DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation</li></ul><h5 id="Multi-branch"><a href="#Multi-branch" class="headerlink" title="Multi-branch"></a>Multi-branch</h5><ul><li>[ICNet] ICNet for Real-Time Semantic Segmentation on High-Resolution Images</li><li>[ContextNet] ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time</li><li>[BiSeNet] BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation</li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li>语义分割论文整理 [<a href="https://zhangbin0917.github.io/2018/09/18/Semantic-Segmentation/" target="_blank" rel="noopener">blog</a>]</li><li>Survey on semantic segmentation using deep learning techniques [<a href="https://www.sciencedirect.com/science/article/pii/S092523121930181X" target="_blank" rel="noopener">paper</a>]</li></ul>]]></content>
    
    
    <categories>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>道路场景语义分割</title>
    <link href="/2019/07/11/%E9%81%93%E8%B7%AF%E5%9C%BA%E6%99%AF%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    <url>/2019/07/11/%E9%81%93%E8%B7%AF%E5%9C%BA%E6%99%AF%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>持续更新<a id="more"></a></p><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><ul><li><a href="http://apolloscape.auto/scene.html" target="_blank" rel="noopener">Apolloscape Scene Parsing</a></li><li><a href="https://bair.berkeley.edu/blog/2018/05/30/bdd/" target="_blank" rel="noopener">BDD100k</a></li><li><a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/" target="_blank" rel="noopener">CamVid</a></li><li><a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener">Cityscapes</a></li><li><a href="http://www.6d-vision.com/scene-labeling" target="_blank" rel="noopener">Daimler Urban Segmentation</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_semantics.php" target="_blank" rel="noopener">Kitti</a></li><li><a href="https://www.mapillary.com/dataset/" target="_blank" rel="noopener">Mapillary Vistas</a></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><div class="table-container"><table><thead><tr><th>Method</th><th>Cityscapes(%)</th><th>CamVid(%)</th><th>Publication</th></tr></thead><tbody><tr><td>FCN-8s</td><td>65.3</td><td></td><td>CVPR2015</td></tr><tr><td>SegNet</td><td></td><td>50.2</td><td>arXiv15</td></tr><tr><td>DeepLab v2</td><td>70.4</td><td></td><td>PAMI2018</td></tr><tr><td>G-FRNet</td><td></td><td>68.0</td><td>CVPR2017</td></tr><tr><td>FRRN B</td><td>71.8</td><td></td><td>CVPR2017</td></tr><tr><td>RefineNet</td><td>73.6</td><td></td><td>CVPR2017</td></tr><tr><td>GCN</td><td>76.9</td><td></td><td>CVPR2017</td></tr><tr><td>TKCN</td><td>79.5</td><td></td><td>ICME2019</td></tr><tr><td>DUC-HDC</td><td>80.1</td><td></td><td>WACV2018</td></tr><tr><td>PSANet</td><td>80.1</td><td></td><td>ECCV2018</td></tr><tr><td>PSPNet</td><td>80.2</td><td></td><td>CVPR2017</td></tr><tr><td>DDSC</td><td></td><td>70.9</td><td>CVPR2018</td></tr><tr><td>DFN</td><td>80.3</td><td></td><td>CVPR2018</td></tr><tr><td>DenseASPP</td><td>80.6</td><td></td><td>CVPR2018</td></tr><tr><td>LDN</td><td>80.6</td><td>78.1</td><td>arXiv19</td></tr><tr><td>GloRe</td><td>80.9</td><td></td><td>CVPR2019</td></tr><tr><td>OCNet</td><td>81.2</td><td></td><td>arXiv18</td></tr><tr><td>DeepLab v3</td><td>81.3</td><td></td><td>arXiv17</td></tr><tr><td>CCNet</td><td>81.4</td><td></td><td>arXiv18</td></tr><tr><td>DAN</td><td>81.5</td><td></td><td>CVPR2019</td></tr><tr><td>HRNetV2</td><td>81.6</td><td></td><td>arXiv19</td></tr><tr><td>CaseNet</td><td>81.9</td><td></td><td>arXiv19</td></tr><tr><td>DeepLab v3+</td><td>82.1</td><td></td><td>ECCV2018</td></tr><tr><td>GFF</td><td>82.3</td><td></td><td>arXiv19</td></tr><tr><td>DPC</td><td>82.7</td><td></td><td>NIPS2018</td></tr><tr><td>GSCNN</td><td>82.8</td><td></td><td>ICCV2019</td></tr></tbody></table></div><h4 id="Real-time-Method"><a href="#Real-time-Method" class="headerlink" title="Real-time Method"></a>Real-time Method</h4><div class="table-container"><table><thead><tr><th>Method</th><th>WxH</th><th>mIOU(%)</th><th>FPS</th><th>GFLOPs</th><th>Param(M)</th></tr></thead><tbody><tr><td>ENet</td><td>1920x1080</td><td>58.3</td><td>21.6</td><td>3.83</td><td>0.37</td></tr><tr><td>SQ</td><td>2048x1024</td><td>59.8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TwoColumn</td><td>1024x512</td><td>72.9</td><td>14.7</td><td>-</td><td>-</td></tr><tr><td>CGNet</td><td>2048x1024</td><td>64.8</td><td>17.6</td><td>6</td><td>0.5</td></tr><tr><td>ESPNet</td><td>-</td><td>60.3</td><td>4</td><td>-</td><td>0.4</td></tr><tr><td>ERFNet</td><td>-</td><td>68</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ICNet</td><td>2048x1024</td><td>69.5</td><td>30.3</td><td>-</td><td>-</td></tr><tr><td>BiSeNet</td><td>1536x768</td><td>68.4</td><td>105.8</td><td>2.9</td><td>5.8</td></tr><tr><td></td><td>1536x768</td><td>74.7</td><td>65.5</td><td>10.8</td><td>49</td></tr><tr><td>ESNet</td><td>-</td><td>70.7</td><td>63</td><td>-</td><td>1.66</td></tr><tr><td>DFANet</td><td>1024x1024</td><td>71.3</td><td>100</td><td>3.4</td><td>7.8</td></tr><tr><td>LEDNet</td><td>1024x512</td><td>70.1</td><td>71</td><td>-</td><td>0.94</td></tr><tr><td>DABNet</td><td>1024x512</td><td>70.1</td><td>104.2</td><td>-</td><td>0.76</td></tr><tr><td>SqueezeNAS</td><td>1024x512</td><td>72.5</td><td>-</td><td>-</td><td>1.9</td></tr></tbody></table></div><p>*上表为文献中各方法在Cityscapes上的性能</p>]]></content>
    
    
    <categories>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>一个测试</title>
    <link href="/2018/05/11/%E4%B8%80%E4%B8%AA%E6%B5%8B%E8%AF%95/"/>
    <url>/2018/05/11/%E4%B8%80%E4%B8%AA%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ヾ(๑╹◡╹)ﾉ”  </p>]]></content>
    
    
    <categories>
      
      <category>胡言</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
